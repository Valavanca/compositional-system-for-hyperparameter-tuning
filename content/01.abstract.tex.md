Introduction. Challenges and Problems.
======================================

Multi-objective optimisation is an established parameter tuning
technique. It is especially suited to solve complex, multidisciplinary
design problems with an accent on system design.

When we talk about several objectives, the intention is to find good
compromises rather than a single solution as in global optimization.
Since the solution for multi-objective optimization problems gives the
appearance to a set of Pareto-optimal points, evolutionary optimization
algorithms are ideal for handling multi-objective optimization problems.

General optimization methods could be classified into derivative and
non-derivative methods. In this thesis focuses on non-derivative
methods, as they are more suitable for parameter tuning. Therefore, they
are also known as black-box methods and do not require any derivatives
of the objective function to calculate the optimum. Other benefits of
these methods are that they are more likely to find a global optimum.

Motivation
----------

The purpose of this study is to introduce new surrogate-design-criteria
for multi-objective hyperparameter optimization software.

Motivation Examples in tuning algorithms:

-   Local search: neighbourhoods, perturbations, tabu length, annealing

-   Tree search: pre-processing, data structures, branching heuristics,
    clause learning deletion

-   Genetic algorithms: population size, mating scheme, crossover,
    mutation rate, local improvement stages, hybridizations

-   Machine Learning: pre-processing, learning rate schedules

-   Deep learning (in addition): layers, dropout constants,
    pre-training, activations functions, units/layer, weight
    initialization

Objectives
----------

Black-box multi-objective problems given a finite number of function
evaluations

Research Questions
------------------

1.  RQ(Cost): Does surrogate-based optimization cheaper in evaluations
    than other multi-goal optimization tools?

2.  RQ(Convergence speed): Does with surrogate-based optimization
    solutions converge faster to Pareto-front than with other multi-goal
    optimization tools?

3.  RQ(Quality): Does surrogate-based optimization return similar or
    better solutions than other optimization tools?

4.  RQ(Extensions and reusability): Reusable compositional system for
    optimization. Is it possible to extend light-weight single-objective
    experiments to heavy-weight multi/many-objective?

In numerous test problems, compositional-surrogate finds comparable
solutions to standard MOEA (NSGA-II, MOEAD, MACO, NSPSO) doing
considerably fewer evaluations (300 vs 5000). Surrogate-based
optimization is recommended when a model is expensive to evaluate.
