Evaluation {#sec:evaluation}
==========

##### Intent:

Performance evaluation Structure:

Experimental setup

:   Optimization problems types, evaluation assumption and budget,
    repetition

Benhmark 1

:   Default Tutor-model(portfolio, thresholds) on plethora types of
    multiobjective problems

    1.  default Tutor-model parameters

    2.  surrogate portfolio items

    3.  Baseline: MOEA

Benhmark 2

:   Parameter selection of Tutor-model with the dynamic sampling plan

    1.  Parameters: prediction count, train/test split, stacking
        solutions. thresholds(x2), solver

    2.  Subset of problems

    3.  Baseline: Static vs Dynamic. Parameters tune

Benhmark 3

:   Many-objective optimization. Objectives&gt;10

    1.  Static Heterogeneous compositional surrogate vs. Homogeneous
        compositional surrogate

    2.  Base line: MOEA or Random

Discussion

:   Results interpretation

In this section, we present the results obtained for proposed methods on
test problems with diverse objective landscape and with a various number
of search variables.

MOEA is called globally convergent if the produced, non-dominated
population converges to the true Pareto front while the number of
generations goes to infinity.

Questions to find out:

-   Advantages and disadvantages of the model-based optimization over
    the classic MOEA

-   Model-based optimization (MBO): compositional vs single surrogate

-   MBO: several identical surrogate vs surrogate portfolio

-   Infill criteria: Selection a point from Pareto-front approximated
    population. Prior vs Posterior

-   Efficacy in handling problems having more than two objectives

For this study, we did not do extensive parameter tuning: NSGA-II and
surrogates were run using their default settings.
[@raw:kouwe2018benchmarking]

Experimental setup
------------------

The measurements were performed on an Intel Core i7-8700CPU machine with
64G of memory using Fedora Server 29.

### Optimization problems

For comparison was selected several widespread synthetic benchmark
suites. All of them are scalable in parameters space and some in
objective space also. They simulate real life problem and have main
related challenges such as multi-modality, different surface type, not
uniform search space, etc.

##### ZDT

This widespread test suite[@raw:ZitzlerDT00] was conceived for two-objective
problems and takes its name from its authors Zitzler, Deb and Thiele.
Each test function involves a particular feature that is known to cause
difficulty in the evolutionary optimization process, mainly in
converging to the Pareto-optimal front (e.g., multimodality and
deception).

-   ZDT1: function has a convex Pareto-optimal front

-   ZDT2: function has a non-convex Pareto-optimal front

-   ZDT3: function adds a discreteness feature to the front. Its
    Pareto-optimal front consists of several noncontiguous convex parts.
    The introduction of a sine function in this objective function
    causes discontinuities in the Pareto-optimal front, but not in the
    parameter space.

-   ZDT4: function has 21 local Pareto-optimal fronts and therefore is
    highly multi-modal

-   ZDT5: integer problem

-   ZDT6: function has a non-uniform search space: the Pareto-optimal
    solutions are non-uniformly distributed along the global Pareto
    front, and also the density of the solutions is lowest near the
    Pareto optimal front and highest away from the front

In their paper the authors propose a set of 6 different scalable
problems all originating from a well thought combination of functions
allowing, by construction, to measure the distance of any point to the
Pareto front

##### DTLZ

This benchmark suite[@raw:DebTLZ05] was conceived for multiobjective
problems with scalable fitness and objective dimensions and takes its
name from its authors Deb, Thiele, Laumanns and Zitzler. All problems in
this test suite are box-constrained continuous n-dimensional
multi-objective problems, scalable in fitness dimension.

-   DTLZ1: The optimal pareto front lies on a linear hyperplane

-   DTLZ2: The search space is continous, unimodal and the problem is
    not deceptive

-   DTLZ3: The search space is continous, unimodal and the problem is
    not deceptive. It is supposed to be harder to converge towards the
    optimal pareto front than DTLZ2

-   DTLZ4: The search space contains a dense area of solutions next to
    the plane

-   DTLZ5: This problem will test an MOEA’s ability to converge to a
    cruve and will also allow an easier way to visually demonstrate
    (just by plotting $f_M$ with any other objective function) the
    performance of an MOEA. Since there is a natural bias for solutions
    close to this Pareto-optimal curve, this problem may be easy for an
    algorithmn to solve. Because of its simplicity its recommended to
    use a higher number of objectives

-   DTLZ6: A more difficult version of the DTLZ5 problem with the
    non-linear distance function g makes it harder to convergence
    against the pareto optimal curve

-   DTLZ7: This problem has disconnected Pareto-optimal regions in the
    search space

##### WFG

This test suite [@raw:WFGref] was conceived to exceed the functionalities of
previously implemented test suites. In particular, non-separable
problems, deceptive problems, truly degenerative problems and mixed
shape Pareto front problems are thoroughly covered, as well as scalable
problems in both the number of objectives and variables. Also, problems
with dependencies between position and distance related parameters are
covered. The WFG test suite was introduced by Simon Huband, Luigi
Barone, Lyndon While, and Phil Hingston. All these problems, were
developed satisfying the following guidelines:

1.  A few unimodal test problems should be present in the test suite.
    Various Pareto optimal geometries and bias conditions should define
    these problems, in order to test how the convergence velocity is
    influenced by these aspects.

2.  The following three Pareto optimal geometries should be present in
    the test suite: degenerate Pareto optimal fronts, disconnected
    Pareto optimal fronts and disconnected Pareto optimal sets

3.  Many problems should be multimodal, and a few deceptive problems
    should also be covered

4.  The majority of test problems should be non-separable

5.  Both non-separable and multimodal problems should also be addressed

-   WFG1: This problems skews the relative significance of different
    parameters by employing different weights in the weighted sum
    reduction. Also, this problem is unimodal and with a convex and
    mixed Pareto optimal geometry

-   WFG2: This problem is non-separable, unimodal and with a convex and
    disconnected Pareto optimal geometry

-   WFG3: This is a non-separable, unimodal problem in all its objective
    except for the last one, which is multimodal

-   WFG4: This is a separable, multimodal problem with a concave Pareto
    optimal geometry. The multimodality of this problem has larger “hill
    sizes” than that of WFG9: this makes it thus more difficult.

-   WFG5: This is a deceptive, separable problem with a concave Pareto
    optimal geometry.

-   WFG6: This problem is non-separable and unimodal. Its Pareto optimal
    geometry is concave. The non-separable reduction of this problem
    makes it more difficult than that of WFG2 and WFG3

-   WFG7: This problem is separable, unimodal and with a concave Pareto
    optimal geometry. This, together with WFG1, is the only problem that
    is both separable and unimodal.

-   WFG8: This is a non-separable, unimodal problem with a concave
    Pareto optimal geometry

-   WFG9: This is a multimodal, deceptive and non-separable problem with
    a concave Pareto optimal geometry. Similar to WFG6, the
    non-separable reduction of this problem makes it more difficult than
    that of WFG2 and WFG3. Also, this problem is only deceptive on its
    position parameters.

Benchmark 1: Portfoli with compositional surogates. RQ1
-------------------------------------------------------

### Surrogate portfolio

Benchmark 2: Dynamic sampling plan. RQ1
---------------------------------------

### Model-tutor parameters

-   Surrogate models

    -   Surrogate portfolio

    -   Surrogates configurations

    -   Validation threshold

    -   Train/test split. Smallest dataset size

-   Solver

    -   MOEA (Population/Generation, Parameter control)

    -   Scalarization with single-optimization algorithms

    -   Random

-   Sampling strategies

-   Pareto front infill criteria (Prior/Posterior)

-   Solutions combination

-   Prediction count

Benchmark 3: Many-objective optimization. Scaling. RQ3
------------------------------------------------------

Compositional surrogates

Discussion
----------

Up to now, most papers used the The quality of the results obtained with
X was similar to the results obtained with Y, but with significantly
fewer exactly evaluated solutions during the optimization process.

##### Neuroevolution of augmenting topologies

\*Training Neural Networks (especially deep ones) is hard and has many
issues (non-convex cost functions - local minima, vanishing and
exploding gradients etc.).

Training Neural Networks (NNs) with Genetic Algorithms (GAs) is not only
feasible, there are some niche areas where the performance is good
enough to be used frequently. A good example of this is Neuroevolution
of augmenting topologies or NEAT, which a successful approach to
generating controllers in simple environments, such as games.

In the more general case though, the approach does not scale well to
large, deep networks with many parameters to tune.

Genetic algorithms and other global searches for optimal parameters are
robust in ways that gradient-based algorithms are not. For instance, you
could train a NN with step function activations, or any other
non-differentiable activation functions. They have weaknesses elsewhere.
One thing relevant in the case of GAs used for NNs, is that weight
parameters are interchangeable in some combinations but heavily
co-dependent in other combinations. Merging two equally good neural
networks with different parameters - which you would do in cross-over in
a GA - will usually result in a third network with poor performance.
NEAT’s success is partially in finding a way to address that issue by
“growing” the NN’s connections and matching them up between similar
neural networks.

Gradient-based approaches are much more efficient. In general, and not
just in domain of NNs, if you can calculate gradient of a function with
respect to parameters, then you can find optimal parameters faster than
most other optimising techniques. An accurate gradient guarantees at
least a small improvement from a single evaluation, and most other
optimisers fall into a generate-and-retry paradigm which cannot make
that kind of guarantee. The weakness of tending to find local optima has
turned out not be a major hindrance for the loss functions in NNs, and
has been tackled with some degree of success using extensions to basic
gradient descent such as momentum, RPROP, Adam etc.

In practice on a large multi-layer network, gradient methods are likely
orders of magnitude faster than GA searches such as NEAT for finding
network parameters. You won’t find any GA-trained CNNs that solve
ImageNet, or even MNIST, where the GA has found the network weights
unaided. However, GAs, or at least some variants of them, are not 100’%’
ruled out. For instance this 2017 blog reviews recent papers including
Large-Scale Evolution of Image Classifiers which explores using GAs to
discover NN hyperparameters which is an important task in machine
learning, and not very tractable using gradient-based methods.
