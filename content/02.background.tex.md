Foundation
==========

Describe general objectives and there constraints

This section explains all the technical terms in the last paragraph. To
start with, we say that optimizers seek candidate(s) x such that it is
unlikely that there exists better candidate(s) y. Each candidate ci is a
set of decisions mizers in this article assume the existence of some
fitness and their associated objective scores; The optimizers in this
article assume the existence of some fitness function f that converts
decisions to output objectives; i.e. Note that this paper uses the term
“model” as a synonym for the fitness function f. Also, the terms single-
and multi- objective optimization apply when joj 1 and joj &gt; 1,
respectively.

Parameter tuning
----------------

Given recent advances in computing hardware, software analysts either
validate these models or find optimal solutions by using automatic tools
to explore thousands to mil- lions of inputs for their systems. Valerdi
notes that, without automated tools, it can take days for human experts
to review just a few dozen examples \[6\]. In that same time, an
automatic tool can explore thousands to millions to billions more
solutions. People find it an overwhelming task just to certify the
correctness of conclusions generated from so many results.

“..for industrial problems, these algorithms generate (many) solutions,
which makes the tasks of under- standing them and selecting one among
them difficult and time consuming” \[1\].

Parameter Types Continuous, integer, ordinal Categorical: finite domain,
unordered, e.g., [apple, tomato, pepper]{}

- Parameter space has structure. Sometimes, some combinations of
parameter settings are forbidden - Configurations often yield
qualitatively different behavior

Define an objective function:

-   Accuracy

-   Runtime

-   Latency

-   Energy

Optimization procedures:

-   Grid search

-   Random search

-   Heuristics

Bayesian methods differ from random or grid search in that they use past
evaluation results to choose the next values to evaluate. Limit
expensive evaluations of the objective function by choosing the next
input values based on those that have done well in the past.

Optimization cost:

-   Evaluation may be very expensive

-   Sampling budget

-   Possibly noisy

-   Feasibility constraints

-   Multi-objective

Ideally, we want a method that can explore the search space while also
limiting evaluations of poor hyperparameter choices.

In parameter tuning approaches, a single criterion may not be sufficient
to correctly characterize the behavior of the configuration space under
consideration and multiple criteria have to be considered.

One way to simplify the task of understanding the space of possible
solutions is to focus on the Pareto frontier; i.e. the subset of
solutions that are not worse than any other (across all goals) but
better on at least one goal. The problem here is that even the Pareto
frontier can be too large to understand. Harman cautions that many
frontiers are very crowded; i.e. contain thousands (or more) candidate
solutions \[7\].

When choosing an optimizer, it is useful to consider what information an
optimizer can access about the function f. For example, gradient descent
optimizers \[16\] need to access the contours around every decision.
This limits the kinds of functions they can process to those with
continuous differential functions (i.e. functions of real-valued
variables whose derivative exists at each point in its domain).

Multi-objective optimization
----------------------------

Parameter tuning is present in our daily life and comes in a variety of
states. The goal is the rich best possible objective by correctly
choosing the system parameters. Common of optimization problems requires
the simultaneous optimization of multiple, usually contradictory,
objectives. These type of problems are termed as multiobjective
optimization problems. The solution to such problems is a family of
points, that placing on a Pareto front.

“Multi-objective optimization deals with such conflicting objectives. It
provides a mathematical framework to arrive at optimal design state
which accommodates the various criteria demanded by the application. The
process of optimizing systematically and simultaneously a collection of
objective functions are called multi-objective optimization (MOO)
[@raw:odugod2013]”. Thus, a central issue of MOO is the relationship between
the objective functions. These are usually modeled as preferences of the
decision maker.

Search or design space(Input space) -&gt; Objective space(Output space).
Practical optimization problems usually involve simultaneous
optimization of multiple conflicting objectives with many
constraints(?).

Knowledge of the Pareto front enables the decision maker to visualize
the consequences of his/her choices in terms of performance for a
criterion at the expense of one or other criteria, and to make
appropriate decisions. Formally, a feasible vector x is said to
(Pareto)-dominate another feasible vector x? if x is at least as good as
x? for all the objectives, and strictly better than x? for at least one
objective. The decision vectors in the feasible set that are not
dominated by any other feasible vector are called Pareto optimal. The
set of non-dominated points in the feasible is the set of Pareto
solutions, whose images (by the objective functions) constitute the
Pareto front.[@raw:Audet2018PerformanceII]

### Scalarizing. Weighted sum methods

Real problems are generally characterized by the presence of many often
conflicting and contradictory objectives. How different objectives
should be combined to yield a final solution? There is also the question
of how to search for an optimal solution to the design problem. The
weighted-sum function approach is a method used to simplify a
multiobjective problem, concatenate the objectives into one criterion by
using weighted sum factors. The merged objective f is used to evaluate
and define the optimal solution.

One approach which is built on the traditional techniques for generating
trade-off surfaces is to aggregate the objectives into a single
parameterized objective function.

Intuitively, Multi-Objective Optimization (MOO) could be facilitated by
forming an alternative problem with a single, composite objective
function using weighted sum. Single objective optimization techniques
are then applied to this composite function to obtain a single optimal
solution. However, the weighted sum methods have difficulties in
selecting proper weight factors especially when there is no articulated
a priori preference among objectives. Indeed, a posteriori preference
articulation is usually preferred, because it allows a greater degree of
separation between the optimization methodology and the decision-making
process which also enables the algorithmic development process to be
conducted independently of the application (Giagkiozos et al, 2015).
Furthermore, instead of a single optimum produced by the weighted sum
approach, MOO can yield a set of solutions exhibiting explicitly the
tradeoff between different objectives.
[@DBLP:journals/corr/abs-1812-07958]

Given two candidates x; y:? Then x:oi ? y:oi and x:oi ? y:oi is true if
objective x:oi ? Each with objectives x:oi; y:oi for 1 ? i ?joj is
(worse,better) than y:oi, respectively. to test is one candidate is
“better” than another. For multiobjective optimization, determining
“better” is somewhat more complicated Traditionally, the space of
candidates with multiple objectives was explored by assigning magic
weights to the objectives, then using an aggregation function to
accumulate the results. Such solutions may be brittle; i.e. they change
dramatically if we alter the magic weights of the objectives.

Why is the Weighting Method Ineffective?\[Hirotaka Nakayama\] Namely, it
can not provide a solution among sunken parts of Pareto surface due to
“duality gap” for not convex cases. Even for convex cases, for example,
in linear cases, even if we want to get a point in the middle of line
segment between two vertices, we merely get a vertex of Pareto surface,
as long as the well known simplex method is used. This implies that
depending on the structure of problem, the linearly weighted sum can not
necessarily provide a solution as DM desires.

### Multy-Objective Evolutionary Algorithms

The term evolutionary algorithm (EA) stands for a class of stochastic
optimization methods that simulate the process of natural evolution.
Using strong simplifications, this set is subsequently modified by the
two basic principles: selection and variation. While selection mimics
the competition for reproduction and resources among living beings, the
other principle, variation, imitates the natural capability of creating
”new” living beings by means of recombination and mutation.

In particular, they possess several characteristics that are desirable
for problems involving i) multiple conflicting objectives, and ii)
intractably large and highly complex search spaces.

Multi-objective Evolutionary Algorithms (MOEAs) are common tools to
solve optimization problems, because of their applicability to complex
fitness landscapes and solid performance on problems with large design
spaces. While other methods also exist, in this thesis we will focus on
approaches using Evolutionary Algorithms for the Multy-objective
optimizations.

However, MOEAs still need many evaluations of the “black box” system to
solve a typical real-world problem. This is further complicated by the
fact that many such problems are very expensive. Consolidated, this
makes MOEAs unfeasible for costly and Multy-objective problem. A good
solution is the integration of the surrogate model which extrapolate and
approximate the fitness landscape from samples. MOEA use this surrogate
model as a target for optimization. Assumed that solution from surrogate
close to a real solution.

We want to understand if the performance of MOEAs approach can be
improved by using compositional surrogates. The key idea of
compositional surrogates is the splitting objective space to multiple
surrogates that extrapolate it independently. Combination of multiple
hypotheses should give them the potential to approximate more
complicated problems.

The various surrogates are analysed on problems of differing complexity,
from simple unimodal problems to problems with difficult multimodal.
Generating a cloud of candidates may be computationally expensive.

-   Quality and Effort tradeoff for multi-objective

-   Human in the loop: Composition technic as tools for domain expert

These algorithms eschew the idea of single solutions, preferring instead
to use the domination function to map out the terrain of all useful
candidates.

Evolutionary optimizers explore populations of candidate solutions. In
each generation some mutator makes changes to the current population. A
select operator then picks the best mutants which are then combined in
some manner to become generation i 1. This century, there has been much
new work on multi-objective evolutionary algorithms with two or three
objectives (as well as many-objective optimization, with many more
objectives). Recently, there has been much interest in applying MOEAs to
many areas of software engineering including requirements engineering,
test case planning, software pro- cess planning, etc. This search-based
software engineering is a rapidly expanding area of research and a full
survey of that work is beyond the scope of this paper (for extensive
notes on this topic, see \[18\], \[21\]) [@raw:MlakarPTF15].

##### Evolutionary Computation

Generating the Pareto set can be computationally expensive and is often
in- feasible, because the complexity of the underlying application
prevents exact methods from being applicable. For this reason, a number
of stochastic search strategies such as evolutionary algorithms, tabu
search, simulated annealing, and ant colony optimization have been
developed: they usually do not guarantee to identify optimal trade-offs
but try to find a good approximation, i.e., a set of solutions whose
objective vectors are (hopefully) not too far away from the optimal
objective vectors [@raw:EmmerichD18].

### Metrics for multy-objective solution

In single-objective minimization, the quality of a given solution is
trivial to quantify: the smaller the objective function value, the
better. However, evaluating the quality of an approximation of a Pareto
set is non trivial. The question is important for the comparison of
algorithms or prediction next configuration. According

According to [@raw:ZitzlerDT00], a Pareto front approximation should satisfy
the following:

-   The distance between the Pareto front and its approximation should
    be minimized.

-   A good (according to some metric) distribution of the points of the
    approximated front is desirable.

-   The extent of the approximated front should be maximized, i.e., for
    each objective, a wide range of values should be covered by the
    non-dominated points.

The goal of metric is to answer this questions.

Metrics for performance indicators partitioned into four groups
according to their properties [@raw:Audet2018PerformanceII]:

-   cardinality

-   convergence

-   distribution

-   spread

Keep making algorithmic progress toward the Pareto front in the
objective function space. Straightforward applying of the coefficient of
determination (R2) is the wrong indicator of success.

Evaluations of different sets of Pareto optimal points is
multi-objective task. Objectives for improving pareto optimal solutions:
- Keep hypervolume low. Reference point is 0 for all objectives -
Maximize sparsity of points. Average distance. Crowding Distance -
Percentage of non-dominant decisions in the total population

Distribution and spread indicators According to [@raw:CustodioMVV11], “the
spread metrics try to measure the extents of the spread achieved in a
computed Pareto front approximation”. They are not really useful to
evaluate the convergence of an algorithm, or at comparing algorithms.
They only make sense when the Pareto set is composed of several
solutions.

For multi-objective optimization (MOO), an algorithm should provide a
set of solutions that realize the optimal trade-offs between the
considered optimization objectives, i.e., Pareto set. Therefore, the
performance comparison of MOO algorithms is based on their Pareto sets.
In this study, two popular metrics Spacing metric(SM) and Hypervolume
(HV) are used to quantify the performance of the algorithms.
[@DBLP:journals/corr/abs-1812-07958]

##### Pareto front evaluation

-   Hypervolume (HV)[@raw:Zitzler2000ComparisonOM]. This metric represents
    the volume of the objective space that is covered by the individuals
    of a non-dominated solutions set (solutions that belong to a Pareto
    front). The volume is delimited by two points: one point that is
    called the anti-optimal point (A) that is defined as the worst
    solution inside the objective space, and a second optimal point
    (pseudo-optimal) that is calculated by the proposed solution method.
    Determining the hypervolume indicator is a computationally expensive
    task. Even in case of a reasonably small dimension and low number of
    points (e.g. 100 points in 10 dimensions), there are currently no
    known algorithms that can yield the results fast enough for use in
    most multiple-objective optimizers.

-   Hyper-area Ratio (HR). The Hyper-area Ratio (HR) \[24\] employs the
    hypervolume of a solution set A divided by the hypervolume value of
    a Reference Front B. Higher values are preferred to lower ones.

-   Pareto Dominance Indicator (ER). Considers the solutions
    intersection between two given sets A and B, which can be provided
    by different algorithms or used to compare a solution set S with a
    Pareto Front P.

-   Crowding Distance. \*pygmo2. The crowding distance value of a
    solution provides an estimate of the density of solutions set.

-   Spacing [@raw:Schott1995FaultTD]. Base on this metrics we can say what
    distribution of Pareto points. Less space metrics means better
    coverage of population target objectives.

Variants in evaluation of sets of solutions for each hypothesis. Each
hypothesis have quality metrics. Solution(s) from each hypothesis have
also own metrics.

The goal of optimizing an multy-objective problem is to obtain an
approximation set A to the PF, including the following two subgoals: (1)
All solutions in A are as close as possible to the PF, (2) All solutions
in A are as diverse as possible in the objective space (3) Evaluate as
few configurations as possible

There are main approaches how produce single solution:

-   Solution from best hypothesis. Sorting

-   Bagging solution

-   Voting solution

##### Designing a Sampling Plan

- The most straightforward way of sampling a design space in a uniform
fashion is by [@raw:EngSurMod] means of a rectangular grid of points. This
is the full factorial sampling technique referred - Latin Squares

##### Conclusion

For optimization expensive black-box:

-   Scalable algorithms that convert multi-objective to single objective
    problem produce solution that not accurate enough(Scalarizing). Also
    this approach suitable for a limited type of problem.

-   Genetic algorithms. This approach is costly to perform and not
    appropriate for expensive problems.

Optimization gap in obtaining high quality, multi/single-obj solutions
in expensive to evaluate experiments. Experiments as a black box,
derivative-free. Reference to surrogate optimization.

Surrogate optimization
----------------------

To dealing with expensive optimization problem more quickly, we can use
surrogate models in the optimization process to approximate the
objective functions of the problem. Approximation of solution is faster
than the whole optimization process can be accelerated. Nevertheless,
the extra time needed to build and update the surrogate models during
the optimization process.

In the literature the term surrogate model (sometimes also meta-model)
based optimization is used where, during the optimization processes,
some solutions are not evaluated with the original objective function,
but are approximated using a model of this function. Different modeling
methods are used to build the surrogate models. For single and
multiobjective optimization similar methods are used. These methods
typically return only one approximated value, which is why in
multiobjective problems several models have to be used, so that every
model approximates one objective. Some of the most commonly used methods
are the Response Surface Method \[2\], Radial Basis Function \[3\],
Neural Network \[4\], Kriging \[5\] and Gaussian Process Modeling \[6,
7, 8\]. In singleobjective optimization the usage of surrogate models is
well established and has proven to be successful. In the literature many
different algorithms and various modeling techniques are used to solve
real-world and benchmark problems \[9, 10\]). The results typically show
that the surrogate-model-based optimization in comparison with
optimization without surrogates provides comparable results in fewer
objective function evaluations \[11, 12\]. Within surrogate-model-based
optimization algorithms a mechanism is needed to find a balance between
the exact and approximate evaluations. In evolutionary algorithms this
mechanism is called evolution control \[13\] and can be either fixed or
adaptive. In fixed evolution control the number of exact function
evaluations that will be performed during the optimization is known in
advance. Fixed evolution control can be further divided into
generation-based control, where in some generations all solutions are
approximated and in the others they are exactly evalu- ated \[14\], and
individual based control, where in every generation some (usually the
best) solutions are exactly evaluated and others approximated \[15\]. In
adaptive evolution control, the number of exactly evaluated solutions is
not known in advance, but depends on the accuracy of the model for the
given problem. Adaptive evolution control can be used in one of two
ways: as a part of a memetic search or to pre-select the promising
individuals which are then exactly evaluated \[16\]. [@raw:MlakarPTF15]

Surrogate used to expedite search for global optimum. Global accuracy of
surrogate not a priority. Surrogate model is cheaper to evaluate than
the objective.

Bayesian optimization (BO) methods often rely on the assumption that the
objective function is well-behaved, but in practice the objective
functions are seldom well- behaved even if noise-free observations can
be collected. We propose to address the issue by focusing on the well-
behaved structure informative for search while ignoring detrimental
structure that is challenging to model data efficiently.
\[arXiv:1906.11152v2\]

robust surrogate models

In [@raw:KrallMD15] proposed approaches that apply kind of surrogate
assistant to evaluations and ranging new population. It allows detect
the most informative examples in population and evaluate them.
identifies and evaluates just those most informative examples In the end
done less evaluations of real system.Another way to explore solutions is
to apply some heuristic to decompose the total space into many smaller
problems, and then use a simpler optimizer for each region. For Another
way to explore solutions is to apply some heuristic to decompose the
total space into many smaller problems, and then use a simpler optimizer
for each region.

GP-DEMO [@raw:MlakarPTF15] The algorithm is based on the newly defined
relations for comparing solutions under uncertainty. These relations
minimize the possibility of wrongly performed comparisons of solutions
due to inaccurate surrogate model approximations. Using this confidence
interval, we define new dominance relations that take into account this
uncertainty and propose a new concept for comparing solutions under
uncertainty that requires exact evaluations only in cases where more
certainty is needed. surrogate-model-based MOEA.

Kind of extend search stage of MOEA with surrogate to simulate
evaluation of population. It transform problem of searching new better
population to improving general hypothesis how and where Pareto set
presented.

We could descreibe compositional-based surrogate optimization as
“compound”\[ref Asman\] gray-box system box black-box optimization whit
a lot of open research areas where surroagte shound improved, managing
portfolio, compare of predictions Pareto fronts, As a developer you can
focused on specific problem and don’t now how implement other
components. this is one of the main advantage to the described approach.

In surrogate-model-based multiobjective optimization, approximated
values are often mistakenly used in the solution comparison. As a
consequence, exactly evaluated good solutions can be discarded from the
population because they appear to be dominated by the inaccurate and
over-optimistic approximations. This can slow the optimization process
or even prevent the algorithm from finding the best solutions
[@raw:MlakarPTF15].

Surrogates are also used to rank and filter out offspring according to
Pareto-related indicators like the hypervolume \[20\], or a weighted sum
of the objectives \[21\]. The problem with the methods that use
hypervolume as a way of finding promising solutions is the calculation
time needed to calculate the hypervolume, especially on many objectives.
Another possibility is described in \[22\], where the authors present an
algorithm that calculates only non-dominated solutions or solutions that
can, because of variance, become non-dominated [@raw:MlakarPTF15].

[@raw:EngSurMod]

##### Use cases

Example for each type of optimization. Justification solution.
Conclusion: Design gap in optimization/parameter tuning. Need to
indicate optimization workflow for expensive process/experiments. The
argument(s) why we need new architecture. Reference to composition
architecture.

Surrogate based optimization has proven effective in many aspects of
engineering and in applications where data is “expensive”, or difficult,
to evaluate.

Compositional architecture
--------------------------

##### Compositional surrogates

Can the same single-objective models be equally applied to various types
of problems in multi-/single-objective optimization?

When there is no correlation between the outputs, a very simple way to
solve this kind of problem is to build n independent models, i.e. one
for each output, and then to use those models to independently predict
each one of the n outputs.

Nevertheless, it is likely that the output values related to the same
input are themselves correlated, but an often naive way to build
multiple models to capable of predicting simultaneously all n outputs is
often given good results.

Later research generalized this approach. MOEA/D (multiobjective
evolutionary algorithm based on decompo- sition \[15\]) is a generic
framework that decomposes a multi- objective optimization problem into
many smaller single problems, then applies a second optimizer to each
smaller subproblem, simultaneously

Moreover, if there are more models, their errors can add up, as well as
the time needed to train the models. In memetic algorithms, especially
if the surrogate model is not very accurate, a local optimum can be
found instead of the global optimum [@raw:MlakarPTF15].

In the case of pre-selecting the promising individuals, the surrogate
model is used to find the promising or drop the low-quality individuals
even before they are exactly evaluated, thus reducing the number of
exact evaluations. For example, OEGADO \[19\] creates a surrogate model
for each of the objectives. The best solutions in every objective get
also approximated on other objectives, which helps with finding
trade-off individuals. The best individuals are then exactly evaluated
and used to update the models.

##### Interfaces and Contracts

##### Reusable software

Problem that each optimization framework/library use inner interfaces.
It is necessary to define a standard that implements best practices for
extension libraries [@raw:buitinck2013api].

We introduce new Model-based line for parameter tuning.

Scope of work
-------------

Describe and implement workflow for multi-objective parameter tuning of
derivative free, black-box system. Parameter estimation is costly. The
proposed solutions are also suitable for single-criteria optimization.
Problem Setting.

Goal:

1.  Globally optimize an objective function(s) that is expensive to
    evaluate. Single/Multi-objective parameter tuning

2.  Scalability in optimization objective. Simultaneously. Gradient-free
    evaluation.

3.  Components reuse. Extensibility with other frameworks.

Problem:

1.  A large number of the target black-box evaluations.

2.  Interfaces not unify.

3.  Code duplication.

Solution:

1.  Compositional architecture.

2.  Surrogate optimization.
