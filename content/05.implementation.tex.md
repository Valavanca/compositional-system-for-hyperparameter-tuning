Implementation {#sec:implementation}
==============

##### Intent:

Details of implementation. Requirements: reusable components, unify
interfaces. Duck typing

Structure:

1. Compositional surrogate model

:   heterogeneous model combination on each objective

    1.  Model-union class. Stacking surrogate. Tree composition

2. Surrogates validation

:   When the surrogate model can be helpful?

    1.  Validation workflow. Adaptation from Data science

    2.  Hypothesis portfolio validation and combination

    3.  Stages and thresholds

3. Solvers

:   Optimization algorithms. Solve problem based on surrogate model(s)

Without automated tools, it can take days for experts to review just a
few dozen examples. In that same time, an automatic tool can explore
thousands to millions to billions more solutions. People find it an
overwhelming task just to certify the correctness of conclusions
generated from so many results.

Separation of concerns

Managing complex execution Strategies

Variants in the evaluation of sets of solutions for each hypothesis.
Each hypothesis has quality metrics. Solution(s) from each hypothesis
have also own metrics.

There are main approaches how produce single solution:

-   Solution from best hypothesis. Sorting

-   Bagging solution

-   Voting solution

##### Reusability in parameter tuning

Parameter tuning can be splitted down into steps that are common for the
many/single-objective optimizations. Each step in optimization workflow
has variability via implemented interfaces. Single-objective hypotheses
can be combined for multi-objective optimization with compositional
design. API of metric-learn is compatible with scikit-learn, the leading
library for machine learning in Python. This allows to use all the
scikit-learn routines (for pipelining, model selection, etc) with metric
learning algorithms through a unified interface.

##### Inner interfaces

Supervised learning consists in learning the link between two datasets:
the observed data X and an external variable y that we are trying to
predict, usually called target or labels. Most often, y is a 1D array of
length $n_samples$. All supervised estimators in scikit-learn implement
a fit(X, y) method to fit the model and a predict(X) method that, given
unlabeled observations X, returns the predicted labels y. Using
arbitrary regression models from scikit-learn as surrogates. Problem
that each optimization framework/library use inner interfaces. It is
necessary to define a standard that implements best practices for
extension libraries [@raw:buitinck2013api]. We introduce new Model-based
line for parameter tuning.

Compositional surrogate
-----------------------

Model-union class. Stacking surrogate. Tree composition

Intuition of why RF is a good model: •Good at non-linearity,
multi-modality and non-smoothness. A decision tree is a non-parametric
supervised machine learning method widely used to formalize decision
making processes across a variety of fields. The combination of many
weak regressors (binary decisions) allows approximating highly
non-linear and multi-modal functions with great accuracy. In addition,
random forests naturally deal with categorical and ordinal variables
which are important in computer systems optimization.

Surrogates validation
---------------------

The main task of surrogate model is to be able to generalize to unseen
data. Surrogate model as learning model should generalize examples to
valid hypothesis. Since we cannot immediately check the surrogate
performance on new, incoming data, it is necessary to sacrifice a small
portion of the examples to check the quality of the model on it. In case
if surrogate model have enoughs score (pass metrics threshold) we
consider it valid and could be processed as subject for
inference(prediction).

### Workflow

##### Designing a Sampling Plan

- The most straightforward way of sampling a design space in a uniform
fashion is by [@raw:EngSurMod] means of a rectangular grid of points. This
is the full factorial sampling technique referred - Latin Squares

Random sampling has the downside that for small sample sizes, there is
often signficant clustering of samples, which is not ideal for
interpolation since clustered samples can be wasteful. Instead, often a
better option is to use a Latin hypercube, which enforces a condition
that sample bins may not share the same coordinates for any coordinate
axis

##### Sampling strategy

Oversampling and undersampling in data analysis. Alleviate imbalance in
the dataset. Imbalance in dataset is not always a problem, more so for
optimization tasks.

The main gain for models not to provide best accuracy on all search
space but provide possible optimum regions. Accuracy in prediction
optimal regions or points from there will direct the search in the right
direction.

Predictor variables can legitimately over- or under-sample. In this
case, provided a carefully check that the model assumptions seem valid.

for other set of parameters, and make a choice from more diverse pool of
models.

Workflow stages:

1.  Cross-Validation $\rightarrow$ Validation threshold

2.  Test-set score $\rightarrow$ Score threshold

3.  Optimization algorithm

4.  MO infill criteria

### Hypothesis portfolio

A Surrogate(s) is a simplified hypothesis of the relation between
parameters and objectives space build on examples. The simplifications
are mean to discard the superfluous details that are unlikely to
generalize to new instances. However, to decide what data to discard and
what data to keep, you must make a hypothesis. For example, a linear
model makes the hypothesis that the data is fundamentally linear and
that the distance between the instances and the straight line is just
noise, which can safely be ignored.

If there is no hypothesis about the data, then there is no reason to
prefer one surrogate over any other. For some datasets, the best model
is a linear model, while for other datasets it is a neural network. No
model is a priori guaranteed to work better, this is consequences from
the No Free Lunch (NFL) theorem. The only way to know for sure which
model is best is to evaluate them all. Since this is not possible, in
practice you make some reasonable assumptions about the data and you
evaluate only a few reasonable models. For example, for simple tasks,
you may evaluate linear models with various levels of regularization,
and for a complex problem, you may evaluate various neural networks.

“No Free Lunch” (NFL) theorems demonstrate that if an algorithm performs
well on a certain class of problems then it necessarily pays for that
with degraded performance on the set of all remaining problems
Additionally, the name emphasizes the parallel with similar results in
supervised learning.

1.  You have to try multiple types of surrogate(models) to find the best
    one for your data.

2.  A number of NFL theorems were derived that demonstrate the danger of
    comparing algorithms by their performance on a small sample of
    problems.

As metamodel-based algorithms are generally developed for black box
problems, where characteristics of the problems to be solved are not
known a priori, one can measure the efficiency of an algorithm by its
ability to provide meaningful solutions in a least number of function
evaluations [@raw:SoftSurvey].

A set of models is defined that can form a partial or complete
hypothesis to describe the problem. Also during the increase of the
experiments may change the model that best describes the existing
problem As a result, there is variability for each problem and
configuration step at the same time. A set of hypotheses can solve this
problem but it takes longer time for cross validation.

Workflow stages:

1.  Cross-Validation $\rightarrow$ Validation threshold

2.  Test-set score $\rightarrow$ Score threshold

3.  Surrogate models sort

4.  Optimization algorithm(s)

5.  MO infill criteria

Solvers
-------

Optimization algorithms. MOEA. A Python
platform[@raw:francesco_biscani_2019] to perform parallel computations of
optimisation tasks (global and local) via the asynchronous generalized
island model.

Realization of main MOEA:

-   NSGA2. Non-dominated Sorting Genetic Algorithm

-   MOEA/D. Multi Objective Evolutionary Algorithms by Decomposition
    (the DE variant)

-   MACO. Multi-objective Ant Colony Optimizer.

-   NSPSO.

##### Results

\[ref: Multi-Objective Parameter Configuration of Machine Learning
Algorithms using Model-Based Optimization\] The approach is linked to
the field of surrogate assisted optimizations. In many practical
settings only a restricted budget is spendable. For example, the arise
of Big Data confronts many machine learning techniques with new
expensive parameter configuration problems. A single training of a
Support Vector Machine (SVM) on a data-set containing less than a
million observations can take several hours.
