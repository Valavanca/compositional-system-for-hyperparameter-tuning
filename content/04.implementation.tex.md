Implementation. Development
===========================

Without automated tools, it can take days for experts to review just a
few dozen examples. In that same time, an automatic tool can explore
thousands to millions to billions more solutions. People find it an
overwhelming task just to certify the correctness of conclusions
generated from so many results.

Separation of concerns

Managing complex execution Strategies

Variants in the evaluation of sets of solutions for each hypothesis.
Each hypothesis has quality metrics. Solution(s) from each hypothesis
have also own metrics.

There are main approaches how produce single solution:

-   Solution from best hypothesis. Sorting

-   Bagging solution

-   Voting solution

##### Designing a Sampling Plan

- The most straightforward way of sampling a design space in a uniform
fashion is by [@raw:EngSurMod] means of a rectangular grid of points. This
is the full factorial sampling technique referred - Latin Squares

Random sampling has the downside that for small sample sizes, there is
often signficant clustering of samples, which is not ideal for
interpolation since clustered samples can be wasteful. Instead, often a
better option is to use a Latin hypercube, which enforces a condition
that sample bins may not share the same coordinates for any coordinate
axis

Dependencies
------------

Adapted to provide base implementation for stages in parameter tuning
with multi-objective

##### Pagmo2

A Python platform [@raw:francesco_biscani_2019] to perform parallel
computations of optimisation tasks (global and local) via the
asynchronous generalized island model. All test suites and basic
multi-objective solvers:

Realization of main MOEA:

-   NSGA2. Non-dominated Sorting Genetic Algorithm

-   MOEA/D. Multi Objective Evolutionary Algorithms by Decomposition
    (the DE variant)

-   MACO. Multi-objective Ant Colony Optimizer.

-   NSPSO.

Tests suits:

-   ZDT [@raw:ZitzlerDT00] is 6 different two-objective scalable problems
    all beginning from a combination of functions allowing, to measure
    the distance of any point to the Pareto front while creating
    problems.

-   WFG [@raw:WFGref] was conceived to exceed the functionalities of
    previously implemented test suites. In particular, non-separable
    problems, deceptive problems, truly degenerative problems and mixed
    shape Pareto front problems are thorougly covered, as well as
    scalable problems in both the number of objectives and variables.
    Also, problems with dependencies between position and distance
    related parameters are covered. In their paper the authors identify
    the need for nonseparable multimodal problems to test
    multi-objective optimization algorithms. Given this, they propose a
    set of 9 different scalable multi-objective unconstrained problems.

-   DTLZ [@raw:DebTLZ05]. All problems in this test suite are
    box-constrained continuous n-dimensional multi-objective problems,
    scalable in fitness dimension.

Reusability in parameter tuning
-------------------------------

Parameter tuning can be splitted down into steps that are common for the
many/single-objective optimizations. Each step in optimization workflow
has variability via implemented interfaces. Single-objective hypotheses
can be combined for multi-objective optimization with compositional
design.

API of metric-learn is compatible with scikit-learn, the leading library
for machine learning in Python. This allows to use all the scikit-learn
routines (for pipelining, model selection, etc) with metric learning
algorithms through a unified interface.

Surrogate hypothesis portfolio
------------------------------

A Surrogate(s) is a simplified hypothesis of the relation between
parameters and objectives space build on examples. The simplifications
are mean to discard the superfluous details that are unlikely to
generalize to new instances. However, to decide what data to discard and
what data to keep, you must make a hypothesis. For example, a linear
model makes the hypothesis that the data is fundamentally linear and
that the distance between the instances and the straight line is just
noise, which can safely be ignored.

If there is no hypothesis about the data, then there is no reason to
prefer one surrogate over any other. For some datasets, the best model
is a linear model, while for other datasets it is a neural network. No
model is a priori guaranteed to work better, this is consequences from
the No Free Lunch (NFL) theorem. The only way to know for sure which
model is best is to evaluate them all. Since this is not possible, in
practice you make some reasonable assumptions about the data and you
evaluate only a few reasonable models. For example, for simple tasks,
you may evaluate linear models with various levels of regularization,
and for a complex problem, you may evaluate various neural networks.

“No Free Lunch” (NFL) theorems demonstrate that if an algorithm performs
well on a certain class of problems then it necessarily pays for that
with degraded performance on the set of all remaining problems
Additionally, the name emphasizes the parallel with similar results in
supervised learning.

1.  You have to try multiple types of surrogate(models) to find the best
    one for your data.

2.  A number of NFL theorems were derived that demonstrate the danger of
    comparing algorithms by their performance on a small sample of
    problems.

As metamodel-based algorithms are generally developed for black box
problems, where characteristics of the problems to be solved are not
known a priori, one can measure the efficiency of an algorithm by its
ability to provide meaning- ful solutions in a least number of function
evaluations [@raw:SoftSurvey].

A set of models is defined that can form a partial or complete
hypothesis to describe the problem. Also during the increase of the
experiments may change the model that best describes the existing
problem As a result, there is variability for each problem and
configuration step at the same time. A set of hypotheses can solve this
problem but it takes longer time for cross validation.

##### Inner interfaces

Supervised learning consists in learning the link between two datasets:
the observed data X and an external variable y that we are trying to
predict, usually called target or labels. Most often, y is a 1D array of
length $n_samples$. All supervised estimators in scikit-learn implement
a fit(X, y) method to fit the model and a predict(X) method that, given
unlabeled observations X, returns the predicted labels y.

Using arbitrary regression models from scikit-learn as surrogates

Problem that each optimization framework/library use inner interfaces.
It is necessary to define a standard that implements best practices for
extension libraries [@raw:buitinck2013api]. We introduce new Model-based
line for parameter tuning.

Validate hypothesis
-------------------

The main task of learning algorithms is to be able to generalize to
unseen data. Surrogate model as learning model should generalize
examples to valid hypothesis. Since we cannot immediately check the
surrogate performance on new, incoming data, it is necessary to
sacrifice a small portion of the examples to check the quality of the
model on it. n case if surrogate model have enoughs score (pass metrics
threshold) we consider it valid and could be processed as subject for
inference(prediction).

### Sampling strategy

Oversampling and undersampling in data analysis. Alleviate imbalance in
the dataset. Imbalance in dataset is not always a problem, more so for
optimization tasks.

The main gain for models not to provide best accuracy on all search
space but provide possible optimum regions. Accuracy in prediction
optimal regions or points from there will direct the search in the right
direction.

Predictor variables can legitimately over- or under-sample. In this
case, provided a carefully check that the model assumptions seem valid.

for other set of parameters, and make a choice from more diverse pool of
models.

Results
-------

\[ref: Multi-Objective Parameter Configuration of Machine Learning
Algorithms using Model-Based Optimization\] The approach is linked to
the field of surrogate assisted optimizations. In many practical
settings only a restricted budget is spendable. For example, the arise
of Big Data confronts many machine learning techniques with new
expensive parameter configuration problems. A single training of a
Support Vector Machine (SVM) on a data-set containing less than a
million observations can take several hours.
