Related work {#sec:related}
============

##### Intent:

This section overview other studies in the area of surrogate-based
multi-objective optimization or related approach from other types of
optimization. The gaps of the others should be clearly. Structure:

Comparison criteria

:   Surrogate type, Surrogate portfolio, Solver type, Sampling size,
    Many-objective optimization

Short description

:   

Comparable table

:   

Many existing approaches can be categorized as multi-objective
optimization. That is why introduce comparison criteria for a clear and
concise demarcation of the approach presented in this thesis:

Comparison criteria for related work:
-------------------------------------

Surrogate type

:   Extrapolation technique for a surrogate model. Surrogate combination

Sampling plan

:   Collecting sampling points for building a surrogate model

Optimization

:   Algorithm to find a multi-objective solution(s)

Scalability

:   Many-objectivity. Problems with high dimensionality in objective and
    parameter spaces

Important Features: Categorical variables, prior knowledge,
multi-objective, feasibility constraints.

Hear presented related works that answer questions related to the
motivation of the thesis, viz the Pareto front approximation objectives
for an expensive black-box function evaluation

Sequential model-based optimization (SMBO) \[3\] has become the
state-of-the-art optimization strategy in recent years. The generic SMBO
procedure starts with an initial design of evaluation points, and then
iterates the following steps: 1. Fit a regression model to the outcomes
and design points obtained so far, 2. query the model to propose a new,
promising point, often by optimizing a so-called infill criterion or
acquisition function, 3. evaluate the new point with the black-box
function and add it to the design. Several adaptations and extensions,
e.g., multi-objective optimization \[4\], multi- point proposal \[5,
6\], more flexible regression models \[7\] or alternative ways to
calculate the infill criterion \[8\] have been investigated recently.

We will briefly present an overview of available software for
model-based optimization, starting with implementations based on the
Efficient Global Op- timization algorithm (EGO), i.e., the SMBO
algorithm proposed by Jones et al. \[3\] using Gaussian processes (GPs),
and continue with extensions and alterna- tive approaches.

We will argue that the proposed concept from this thesis is the
preferred choice for functional optimization when the evaluation cost is
large.

### Platforms and frameworks

There are a lot of different projects that can handle multi-objective
solutions.

General characteristics are that they have multiple algorithms on each
type of optimization and additional features that they provide. Usually,
a surrogate model is predefine and injected into some algorithm for
decision making.

##### raw:PlatEMO

[@raw:PlatEMO] raw:PlatEMO: A MATLAB Platform for Evolutionary Multi-Objective
Optimization. The platform provides a plethora of optimization
algorithms for multi-/many objective problems.

### Algorithms. Related Software

##### SigOpt

build on top of cross-validation

##### Sequential Model-based Algorithm Configuration (SMAC)

SMAC[@raw:raw:smac-2017] adopted a random forests model and Expected Improvement
(EI) to model a conditional probability. It applies a local search with
several starting points and picks configurations with maximal EI. The
exploration property of SMAC is improved by EI on points with large
uncertainty and optimal value of objective mean. However, SMAC is
limited to single-criteria optimization and use predefine sampling plan.

##### raw:mlrMBO: A Modular Framework for Model-Based Optimization of Expensive Black-Box Functions

Bischl et al.,[@raw:mlrMBO] provide a framework with a focus on
multi-criteria parameters optimization. MlrBO extends the MBMO procedure
for mixed and hierarchical parameter spaces. For the surrogate, that
project allows any regression learner from $mlr$ library. That is why a
bagging method can be applied to regression models to retrieve model
error in $mlr$. This framework enables proposing several points for
evaluation. However, it doesn’t provide a combination of different
surrogates into one model. Example of provided algorithm: ParEGO

##### ParEGO

scalarization based multi-objective algorithm [@raw:raw:Knowles06]. Classical
single-objective algorithm of Jones et al. EGO was extended to a
multi-point proposal by repeat algorithm several times with randomly
change scalarizations weights in each iteration. The idea of algorithm
based on Kriging/Gaussian process regression model and multiple single
objective optimization processes on scalarized objectives guaranteeing
that multiple points on the Pareto-optimal front are found.

##### An Evolutionary Algorithm with Spatially Distributed Surrogates for Multiobjective Optimization

Amitay et al.,[@raw:raw:DistrSurr]presented in their paper an evolutionary
algorithm with spatially distributed surrogates (EASDS). Surrogates
periodicaly validated and updated. Radial Basis Function Networks were
used as the surrogate model for each partition from samplings points.
Spatially Distributed Surrogate models are created for all the
objectives and than evaluated by NSGA-II. The authors describe that
their approach achieves better results than single global surrogate
models showing an advantage from using multiple surrogates. However,
authors evaluated there algorithm only on bi-objective problems

##### A hybrid surrogate-based approach for evolutionary multi-objective optimization

Rosales-Pérez et al.,[@raw:raw:HybridSurrRCG] proposed an approach based on an
ensemble of Support Vector Machines. Authors describe a model selection
process or hyperparameters selection of SVM based on cross-validation
technique and fother injection to the surrogate ensemble. Incremental
development of the ensemble, that includes new information obtained
during the optimization and stores previous models. The training of a
new model carries the grid search of SVM Kernel types to find one with
the least expected generalization error. This paper presents a model
selection process for determining the hyperparameters for each SVM in
the ensemble.

##### Efficient Multi-Objective Optimization through Population-based Parallel Surrogate Search

Akhtar et al.,[@raw:raw:akhtar2019efficient] introduce a multi-objective
optimization algorithm for expensive functions that connect iteratively
updated several surrogates of the objective functions. Feature of this
algorithm is high optimization for parallel computation. An algorithm
combines Radial Basis Function (RBF) approximation, Tabu and local
search around multiple points. Authors present an algorithm that can
theoretically be applicable for hight dimensional space and
many-objective problems.

##### GALE: Geometric Active Learning for Search-Based Software Engineering

Krall et al.,[@raw:raw:KrallMD15] developed algorithm that uses PCA and active
learning techniques to step-by-step approximation and evaluating the
most informative solutions. The main features of GALE are active
learning in geometrically analysis perspective regions in search space
to select most prospective candidates.

##### Hypermapper

Luigi Nardi et al. [@raw:raw:nardi2019practical] presented multi-objective
black-box optimization tool. Some features of their approach are prior
knowledge, categorical variables, feasibility and good adaptation to
embedded devices. They train separate models, one for each objective and
constarints. Then merge it with random scalarizations (Tchebyshev/
Linear scalarization). Next, the Bayesian model leads local search
algorithm to explore Pareto optimal points.

##### Evolutionary optimization with hierarchical surrogates

Xiaofen Lu et al. [@raw:raw:LuST19] apply different surrogate modelling
techniques based on motivation on optimization expensive black-box
function without any prior knowledge on a problem. They used a
pre-specified set of models to construct hierarchical surrogate during
optimization. Also, for verification of surrogate used general accuracy
of the high-level model. The whole process of the proposed method split
to accumulate training samples and model-based optimization, that means
that the sample plan is static and require prior information about the
problem.

                                        Surrogate                                    Optimization Algorithm                               Sampling plan                     Scaling of test problems   Year
  ------------------------------------- -------------------------------------------- ---------------------------------------------------- --------------------------------- -------------------------- ------
  Distributed Surrogates (EASDS)        Radial Basis Function Networks, clustering   NSGA-II                                              Static: Random samples            2 obj                      2007
  Hybrid surrogate-based approach       Ensemble of Support Vector Machines          NSGA-II                                              Static: Latin-hypercube samples   2 obj                      2013
  raw:mlrMBO                                Regression model(s)                          Single-/Multi-opt algorithm                          Static                            ?                          2017
  SMAC                                  Eandom forests + Expected Improvement        Local search                                         Static                            1 obj                      2017
  Parallel Surrogate Search (MOPLS-N)   Radial Basis Function                        Tabu and local search                                Static                            2 obj                      2019
  GALE                                  PCA-approximation + linear models            MOEA/D                                               Static                            2-8 obj                    2015
  Hypermapper                           Randomized decision forests                  Scalarizations + Gaussian Processes + Local search   Static                            2 obj                      2019
  ParEGO                                Kriging/Gaussian process regression          scalarizations + EGO                                 Static: Latin hypercube samples                              2006
  Hierarchical surrogates               Dinamic hierarchical surrogate               EA                                                   Static                            1 obj                      2019

  : Related work

### Conclusions

1.  One modeling technique might perform differently on different
    problem landscapes. [@raw:raw:LuST19]

2.  Surrogate model can be useful when the accuracy is larger than 0.5.
    [@raw:raw:LuST19]
