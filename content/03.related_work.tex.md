Related Work {#sec:related}
============

This section overviews other studies in the area of surrogate-based
multi-objective optimization and related approaches of other types of
optimization.

Comparison criteria
-------------------

Many existing approaches can be categorized as multi-objective
optimization approaches. Therefore, the comparison criteria for a clear
and concise definition of the approach are introduced in this thesis:

-   **Sampling plan** specifies the size of a sample set from which to build a surrogate
    model and the sampling strategy that will pick these samples. The
    sampling plan can be static when decisions about samples are made
    ahead of time or it can be dynamic when they depend on optimization
    success.

-   **Surrogate type** is describe extrapolation models and a composition
    strategy to combine these models. In this context, variability
    indicates that the surrogate model is exchangeable and can be
    selected for a specific problem. The extensibility of a surrogate
    refers to the ability to grow and improve general extrapolations for
    a particular problem.

-   **Optimization algorithm** is applied to find the optimal points in the search space. The
    architecture of the optimization algorithm and the surrogate model
    can be tightly coupled () either when the surrogate model is nested
    in the optimization algorithm, or when they perform flat
    architecture with ( Figure {@fig:surr_opt_architecture}).

    ![Example of a possible combination the optimization algorithm with the surrogate model [@raw:FigueiraA14]](images/utility/architecture_surr_opt.svg){#fig:surr_opt_architecture width=100%}

-   **Scalability** refers to the dimensionality of problems that were applied to
    analyze the performance of the algorithm.

-   **Multi-point proposal** is a property of yielding the required number of multi-objective solutions.

Almost all related works of parameter tuning could be categorized as
[@raw:JonesSW98]. The general looks as follows (Figure {@fig:sequential_mbo}):

1.  Start with the initial sample plan of evaluation points.

2.  Build a regression model to provide a hypothesis about the relation
    between parameters and objectives.

3.  Use the built surrogate model as a parameter for optimization
    strategy. The solutions from the optimization algorithm are new
    promising points for evaluation.

4.  Evaluate the new predicted points and add them to the existing
    samples.

5.  If the stop criteria are not met, repeat optimization with the
    updated sample set.

![General Sequential model-based optimization}](images/utility/sequential_mbo.svg){#fig:sequential_mbo width=100%}

We present an overview of previous work on model-based multi-objective
optimization. We begin the project for model-based optimization (mlrMBO)
and continue with related algorithms including various optimization
improvements.

Platforms and frameworks
------------------------

There are many different projects that can perform multi-objective
optimization. Frameworks provide multiple multi-objective algorithms,
performance metrics, built-in test problems and extra tools such as
plotting and benchmarks. Some frameworks focus on efficient calculations
and parallelization [@raw:francesco_biscani_2019], others on implementation
of modern multi-objective algorithms
[@raw:benitezhidalgo2019jmetalpy; @raw:TianPlatEMO] and on support of plenty of
model-based optimization algorithms [@raw:BischlmlrMBO].

#### mlrMBO

[@raw:BischlmlrMBO] is a modular framework for model-based optimization of
expensive black-box functions. MlrBO extends the procedure to a
multi-objective problem with mixed and hierarchical parameter spaces.
Modular structure and integration with *mlr*[^1] library allows all
regression models to be used and compositional techniques such as
bagging and ensembling to be applied. The authors implemented four
different model-based multi-objective algorithms that are categorized in
three classes: 1) Scalarization-based algorithms that optimize a
scalarize version of the black-box functions (). 2) Pareto-based
algorithms that build individual models for each objective and complete
multi-objective optimization (MSPOT [@raw:ZaeffererBNWE13]). 3) Direct
indicator-based algorithms which also fit individual models, but perform
a single objective optimization on the collection of all models (SMS-EGO
[@raw:inproceedings], $\epsilon$-EGO [@raw:WagEGOe]). A special feature of the
mlrMBO framework is multi-point proposal prediction on each iteration.
However, it does not provide a combination of different surrogate models
into one model.

#### BRISE 2.0 {#alg:BRISE}

[@raw:Pukhkaiev19] is a software product line for parameter tuning. The core
topics of their approach are extensibility and variability with key
features such as 1) A *repeater* which automatically decides on the
number of repetitions needed to obtain the required accuracy for each
configuration. 2) *Stop condition*, which validates a solution received
from the model and decides whether to stop the experiment. 3)
Association of a sampling plan with model prediction validity which
provides a *flexible sampling plan*. The main advantage of this sampling
plan is that it requires less initial knowledge about the optimization
problem. Extrapolation model for parameter prediction is exchangeable
and it combines surrogate and optimization strategies in each
implementation. Several general models, such as polynomial regression
surrogate with local search and Bayesian optimization with bandit-based
methods strategy(BOHB [@raw:FalknerBOHB]), are implemented. However, the
models lack variability and should be designed from scratch for new
domain-problem.

#### SMAC

[@raw:HutterHL11; @raw:smac-2017] is a framework for parameter tuning with
Bayesian Optimization in combination with a simple racing mechanism to
decide which of two configurations performs better. SMAC adopted a
random forest model and Expected Improvement (EI) to model conditional
probability. It applies a local search with several starting points to
pick configurations with a maximal value of EI. These points improve the
exploration possibilities of SMAC in the search space with higher
uncertainty and an optimal value of the objective mean. An interesting
feature of SMAC is its support of the termination of unfavourable
evaluations that are slowing down the optimization process. However,
SMAC is limited to single-criteria optimization and uses a predefined
sampling plan.

#### Hypermapper 2.0

Luigi Nardi et al. [@raw:nardi2019practical] presented a multi-objective
black-box optimization framework that solves complex parameter tuning
problems with the combination of search space, expensive evaluation, and
feasibility constraints. The proposed approach can be classified as a
direct indicator-based algorithm with constraint validation. During this
type of optimization, several identical individual models for each
objective are built, then a single-objective optimization is performed
on the aggregation from all these models. The authors further extended
this idea to predict no feasible configurations with an extra surrogate
model.

The Hypermapper 2.0 is based on a surrogate of a random forest. The
random forest combines several weak regressors on a subset of samples to
yield accurate regression and effective classification models. After
scalarizing values from models, framework applies an Bayesian
optimization (BO) method to find an optimal value. Since using a single
weight vector would only find one point on the Pareto optimal front, a
weight vector is chosen randomly for each iteration, ensuring that
multiple points on the Pareto optimal front are found. The key features
of this approach are the possibility to use prior knowledge, support
real variables, predict feasibility, and excellent final adaptation of
the implementation to embedded devices. The authors reported that
Hypermapper 2.0 provides better Pareto fronts compared to
state-of-the-art baseline, i.e. better competitive quality and saving
evaluation budget.

Model-based multi-objective algorithms
--------------------------------------

Fixed optimization components can make general optimization ineffective
in the face of different problems. The flexibility achieved by the
surrogate construction methodology and multi-objective (MO) algorithms
can help to achieve solutions closest to the real Pareto optimal front.

#### ParEGO {#alg:ParEGO}

is a scalarization-based multi-objective algorithm [@raw:Knowles06]. It was
developed as extension, which can encompass multi-point proposal, of a
classic single-objective algorithm [@raw:JonesSW98] of Jones et al. In its
core lays repetition of an algorithm execution with randomly changed
scalarization weights for each iteration. This algorithm is based on the
Kriging (Gaussian process regression) model and multiple
single-objective optimization processes on scalarized objectives.
Several runs with random weights guarantee that multiple points on the
Pareto optimal front are predicted. This algorithm and its modification
are implemented in mlrMBO[@raw:BischlmlrMBO].

#### An Evolutionary Algorithm(EA) with Spatially Distributed Surrogates

Amitay et al.,[@raw:DistrSurr] presented an with spatially distributed
surrogates (EASDS). Surrogate models use Radial Basis Function Networks,
periodically validating and updating each subset of samplings points.
This generates a complex ensemble surrogate with approximations in local
areas of the search space. Spatially Distributed Surrogate models are
created for all objectives and then evaluated by NSGA-2 [@raw:DistrSurr].
The authors report that their approach achieves better results than
single global surrogate models, multiple surrogates. Of note, the
authors evaluated their algorithm only on bi-objective problems.

#### A hybrid surrogate-based approach for evolutionary multi-objective optimization

Rosales-Pérez et al.,[@raw:HybridSurrRCG] proposed an approach based on an
ensemble of Support Vector Machines (SVM). The authors describe a model
selection process or hyperparameters selection of SVM based on a
validation technique and a further injection into the surrogate
ensemble. Incremental development of the ensemble includes new
information obtained during the optimization and old evidence stored in
previous models. The training of a new model represents search on the
SVM grid in order to find a kernel with the lowest expected
generalization error. This paper presents a model selection process for
determining the hyperparameters for each SVM in the ensemble.

#### Evolutionary optimization with hierarchical surrogates

Xiaofen Lu et al. [@raw:LuST19] apply different surrogate modelling
techniques based on the motivation to optimize expensive black-box
functions without any prior knowledge of the problem. They used a
pre-specified set of models to construct hierarchical surrogates during
optimization. Also, to verify the surrogates, the general accuracy of
the high-level model was used. The process of the proposed method
involves splitting the accumulated training samples and model-based
optimization, which means that the sample plan is static and requires
prior information about the problem.

The authors show that the hierarchical surrogate structure can be
beneficial when the accuracy of the high-level model is greater than
0.5. They also noticed that a single modelling technique might perform
differently on different problem landscapes. This motivates us to use a
pre-specified set of modelling techniques (portfolio with surrogates).

#### Population-based Parallel Surrogate Search

Akhtar et al.,[@raw:akhtar2019efficient] introduce a multi-objective
optimization algorithm for expensive functions that connects several
iteratively updated surrogates of the objective functions. The key
feature of this algorithm is high optimization for parallel computation
and stacking predictions from the set of Radial Basis Function (RBF)
models. The algorithm combines RBF composition surrogate, Tabu, and
local search around multiple points.

#### GALE: Geometric Active Learning for Search-Based Software Engineering

Krall et al.,[@raw:KrallMD15] developed an algorithm that uses principal
components analysis (PCA) and active learning techniques to perform
step-by-step approximation and evaluation of the most informative
solutions. The authors notice that MOEAs are not suitable for expensive
multi-objective problems because they push a set of solutions towards an
outer surface of better candidates, costing many function evaluations.
The fundamental idea of the proposed approach is to choose the most
informative solutions from a large set of options. This is accomplished
by dividing the functional landscape into smaller regions and evaluating
only the most informative samples from the regions. As a result of the
division of the functional landscape, function evaluations are spared
since only a few of the most informative points from the region are
evaluated. A drawback of this approach lays in static implementation
with MOEA/D.



| **Approach**                      	| **Multi-objective** 	| **Sampling plan** 	| **Extrapolation models** 	| **Composition strategy** 	| **Optimization** 	| **Mixed search space** 	| **Multi-point proposal** 	| **Scalability** 	|
|-------------------------------	|:---------------:	|:-------------:	|:--------------------:	|:--------------------:	|:------------:	|:------------------:	|:--------------------:	|:-----------:	|
| mlrMBO [@raw:BischlmlrMBO]                             | ✓               	| static        	| E                    	| V                    	| V            	| ✓                  	| ✓                    	| ✓           	|
| BRISE 2.0 [@raw:Pukhkaiev19]                           | ✗               	| flexible      	| V                    	| V                    	| V            	| ✓                  	| ✓                    	| ✓           	|
| SMAC [@raw:HutterHL11; @raw:smac-2017]                 | ✗               	| static        	| S                    	| S                    	| S            	| ✓                  	| ✗                    	| ✓           	|
| Hypermapper 2.0 [@raw:nardi2019practical]              | ✓               	| static        	| S                    	| S                    	| S            	| ✓                  	| ✗                    	| ✓           	|
| ParEGO [@raw:Knowles06]                                | ✓               	| static        	| S                    	| S                    	| S            	| ✗                  	| ✗                    	| ✓           	|
| Distributed Surrogates, EASDS [@raw:DistrSurr]         | ✓               	| static        	| S                    	| E                    	| S            	| ✗                  	| possible             	| ✗           	|
| Hybrid surrogate [@raw:HybridSurrRCG]                  | ✓               	| static        	| S                    	| E+D                  	| V            	| ✗                  	| possible             	| ✓           	|
| Hierarchical surrogates [@raw:LuST19]                  | ✗               	| static        	| E                    	| V                    	| V            	| ✗                  	| possible             	| ✗           	|
| Parallel surrogates, MOPLS [@raw:akhtar2019efficient]  | ✓               	| static        	| S                    	| E                    	| S            	| ✗                  	| ✓                    	| ✗           	|
| GALE [@raw:KrallMD15]                                  | ✓               	| static        	| S                    	| E                    	| S            	| ✓                  	| possible             	| ✓           	|
The comparison of related approaches. The component behaviour S - static, V - variability, E-extensibility, D - dynamic.

Scope of work
-------------

As shown before, surrogate or model-based optimization suit expensive
black-box problems. A summary of the related works that we have
discussed is shown in the table above. Nevertheless, model-based
approach still has limitations:

-   Multi-objective hypothesis. A *limited number* of surrogate models
    can handle with several parameter and objectives, but they struggle
    when these parameters and objectives become larger.

-   *Surrogate is domain-specific.* Currently, to improve and reach the
    best prediction, we need to know the objective surface in order to
    apply a specific surrogate. Universal surrogates might gain optimal
    results but may not be the most reliable [@raw:abs181207958; @raw:LuST19].

-   The quality of predictions depends on *the number of samples* we use
    for a specific type of surrogate. There is a trade-off between the
    reduction of sample size and maximization of prediction quality.
    Overfitting, as well as underfitting, can guide optimization in a
    wrong direction.

-   Often, optimization algorithms and surrogate models are *coupled*
    extremely tightly. This interdependence makes the general approach
    biased against specific problems. Reimplementation of these
    algorithms for each usage scenario becomes time-consuming and
    error-prone.

After surveying the aforementioned literature, we derive the following
research gaps and questions.

- **Surogate combination**

    Previous research has shown that surrogate model selection can
    profoundly impact the quality of the optimization solution
    [@raw:HybridSurrRCG]. Furthermore, it has been noted that the surrogate
    is domain-specific, and the same technique might perform differently
    on different problems [@raw:LuST19]. The overall research gaps of the
    surrogate model’s flexibility can be divided into the following
    subproblems:

    1.  The dynamic combination of different single-criterion models is
        crucial to solve the multi-criteria problems. It must be
        feasible to substitute the type of nested surrogate model for an
        arbitrary criterion component. This would make the surrogate
        model more versatile and capable of describing arbitrary
        optimization problems. Therefore, technology that would allow
        the creation of surrogate compositional models is necessary.
        Hence we can identify the first research question **\[RQ1\]**:
        Does the dynamic composition of different single-objective
        models improve the extrapolation of multi-objective problems?

    2.  After scrupulous investigation of presented works we conclude
        that access to the range of models improves final results.
        Unfortunately, most authors used only one type of model with
        various hyperparameters. Therefore, the next research question
        **\[RQ2**\]: Does a portfolio of surrogate models enhance
        optimization results?

- **Sampling plan**

    A static plan requires additional knowledge of the surface of the
    problem, which is usually not possible to obtain. Moreover,
    arbitrary decisions of the sample size might be a reason that leads
    to inaccurate models and further wrong results. These problems may
    occur because the surrogate model is overfitted or underfitted, and
    the result is a waste of samples and resources. Consequently, the
    last research question **\[RQ3\]**: Does a dynamic sampling plan
    help accelerate obtaining an optimal solution?

To our knowledge, there have not been studies of the comprehensive
surrogate combinations and the dynamic sampling strategy. Therefore, we
focus on the improvement of compositional surrogate models for
multi-objective problems. We aim to apply surrogate models to problems
that are expensive, multi-objective, and derivative-free/black-box
systems without constraints.

#### The following goals must be achieved:

1.  Find diverse multi-objective solutions with minimal distance to real
    the Pareto front.

2.  Develop modular and extensible architecture.

3.  Improve the backward computationally with single-objective parameter
    tuning.

4.  Reduce the evaluation budget.

[^1]: [mlr]{}: Machine Learning in R. https://mlr.mlr-org.com/
