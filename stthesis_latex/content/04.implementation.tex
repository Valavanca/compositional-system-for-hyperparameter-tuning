\chapter{Implementation. Development}

Without automated tools, it can take days for experts to review just a few dozen examples.  In that same time, an automatic tool can explore thousands to millions to billions more solutions. People find it an overwhelming task just to certify the correctness of conclusions generated from so many results.

Separation of concerns

Managing complex execution Strategies

Variants in the evaluation of sets of solutions for each hypothesis. Each hypothesis has quality metrics. Solution(s) from each hypothesis have also own metrics.
               
There are main approaches how produce single solution: 
\begin{itemize}
    \item Solution from best hypothesis. Sorting
    \item Bagging solution
    \item Voting solution                
\end{itemize}

\paragraph{Designing a Sampling Plan}
 - The most straightforward way of sampling a design space in a uniform fashion is by \cite{EngSurMod}
 means of a rectangular grid of points. This is the full factorial sampling technique referred
 - Latin Squares

% --------------------------------------------------------------------------------------------
% ---------------------------------------------------       Dependencies      ----------------
% --------------------------------------------------------------------------------------------
\section{Dependencies}
    Adapted to provide base implementation for stages in parameter tuning with multi-objective

    \paragraph{Pagmo2} 
        A Python platform \cite{francesco_biscani_2019} to perform parallel computations of optimisation tasks (global and local) via the asynchronous generalized island model.
        All test suites and basic multi-objective solvers:

        Realization of main MOEA:
        \begin{itemize}
            \item NSGA2. Non-dominated Sorting Genetic Algorithm
            \item MOEA/D. Multi Objective Evolutionary Algorithms by Decomposition (the DE variant)
            \item MACO. Multi-objective Ant Colony Optimizer.
            \item NSPSO. 
        \end{itemize}

        Tests suits:
        \begin{itemize}
            \item ZDT \cite{ZitzlerDT00} is 6 different two-objective scalable problems all beginning from a combination of functions allowing, to measure the distance of any point to the Pareto front while creating problems.
            \item WFG \cite{WFGref} was conceived to exceed the functionalities of previously implemented test suites. In particular, non-separable problems, deceptive problems, truly degenerative problems and mixed shape Pareto front problems are thorougly covered, as well as scalable problems in both the number of objectives and variables. Also, problems with dependencies between position and distance related parameters are covered. In their paper the authors identify the need for nonseparable multimodal problems to test multi-objective optimization algorithms. Given this, they propose a set of 9 different scalable multi-objective unconstrained problems.
            \item DTLZ \cite{DebTLZ05}. All problems in this test suite are box-constrained continuous n-dimensional multi-objective problems, scalable in fitness dimension.
        \end{itemize}


% --------------------------------------------------------------------------------------------
% ---------------------------------------------------        Portfolio       ----------------
% --------------------------------------------------------------------------------------------    

\section{Portfolio with hypothesis}
A set of models is defined that can form a partial or complete hypothesis to describe the problem.
Also during the increase of the experiments may change the model that best describes the existing problem
As a result, there is variability for each problem and configuration step at the same time. 
A set of hypotheses can solve this problem but it takes longer time for cross validation.


% --------------------------------------------------------------------------------------------
% ---------------------------------------------------        Validation       ----------------
% -------------------------------------------------------------------------------------------- 
\section{Validate hypothesis}
    \epigraph{``All models are wrong but some are useful``}{\textit{â€“ George Box}}

    The main task of learning algorithms is to be able to generalize to unseen data. Surrogate model as learning model should generalize examples to valid hypothesis. 
    Since we cannot immediately check the surrogate performance on new, incoming data, it is necessary to sacrifice a small portion of the examples to check the quality of the model on it.
    n case if surrogate model have enoughs score (pass metrics threshold) we consider it valid and could be processed as subject for inference(prediction).

    \subsection{Sampling strategy}
    Oversampling and undersampling in data analysis. Alleviate imbalance in the dataset. 
    Imbalance in dataset is not always a problem, more so for optimization tasks. 

    The main gain for models not to provide best accuracy on all search space but provide possible optimum regions.
    Accuracy in prediction optimal regions or points from there will direct the search in the right direction.

    Predictor variables can legitimately over- or under-sample. 
    In this case, provided a carefully check that the model assumptions seem valid.


    for other set of parameters, and make a choice from more diverse pool of models.
