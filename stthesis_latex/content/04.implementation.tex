\chapter{Implementation. Development}

Without automated tools, it can take days for experts to review just a few dozen examples.  In that same time, an automatic tool can explore thousands to millions to billions more solutions. People find it an overwhelming task just to certify the correctness of conclusions generated from so many results.

Separation of concerns

Managing complex execution Strategies

Variants in the evaluation of sets of solutions for each hypothesis. Each hypothesis has quality metrics. Solution(s) from each hypothesis have also own metrics.
               
There are main approaches how produce single solution: 
\begin{itemize}
    \item Solution from best hypothesis. Sorting
    \item Bagging solution
    \item Voting solution                
\end{itemize}

% ---------------------------------------------------       Sampling Plan      ----------------
\paragraph{Designing a Sampling Plan}
 - The most straightforward way of sampling a design space in a uniform fashion is by \cite{EngSurMod}
 means of a rectangular grid of points. This is the full factorial sampling technique referred
 - Latin Squares

 Random sampling has the downside that for small sample sizes, there is often signficant clustering of samples, which is not ideal for interpolation since clustered samples can be wasteful. Instead, often a better option is to use a Latin hypercube, which enforces a condition that sample bins may not share the same coordinates for any coordinate axis

% --------------------------------------------------------------------------------------------
% ---------------------------------------------------       Dependencies      ----------------
% --------------------------------------------------------------------------------------------
\section{Dependencies}
    Adapted to provide base implementation for stages in parameter tuning with multi-objective

    \paragraph{Pagmo2} 
        A Python platform \cite{francesco_biscani_2019} to perform parallel computations of optimisation tasks (global and local) via the asynchronous generalized island model.
        All test suites and basic multi-objective solvers:

        Realization of main MOEA:
        \begin{itemize}
            \item NSGA2. Non-dominated Sorting Genetic Algorithm
            \item MOEA/D. Multi Objective Evolutionary Algorithms by Decomposition (the DE variant)
            \item MACO. Multi-objective Ant Colony Optimizer.
            \item NSPSO. 
        \end{itemize}

        Tests suits:
        \begin{itemize}
            \item ZDT \cite{ZitzlerDT00} is 6 different two-objective scalable problems all beginning from a combination of functions allowing, to measure the distance of any point to the Pareto front while creating problems.
            \item WFG \cite{WFGref} was conceived to exceed the functionalities of previously implemented test suites. In particular, non-separable problems, deceptive problems, truly degenerative problems and mixed shape Pareto front problems are thorougly covered, as well as scalable problems in both the number of objectives and variables. Also, problems with dependencies between position and distance related parameters are covered. In their paper the authors identify the need for nonseparable multimodal problems to test multi-objective optimization algorithms. Given this, they propose a set of 9 different scalable multi-objective unconstrained problems.
            \item DTLZ \cite{DebTLZ05}. All problems in this test suite are box-constrained continuous n-dimensional multi-objective problems, scalable in fitness dimension.
        \end{itemize}


\section{Reusability in parameter tuning}
Parameter tuning can be splitted down into steps that are common for the many/single-objective optimizations. 
Each step in optimization workflow has variability via implemented interfaces.
Single-objective hypotheses can be combined for multi-objective optimization with compositional design.

API of metric-learn is compatible with scikit-learn, the leading library for machine learning in Python. 
This allows to use all the scikit-learn routines (for pipelining, model selection, etc) with metric learning algorithms through a unified interface.


% --------------------------------------------------------------------------------------------
% ---------------------------------------------------        Portfolio        
% --------------------------------------------------------------------------------------------    
\section{Surrogate hypothesis portfolio}
A Surrogate(s) is a simplified hypothesis of the relation between parameters and objectives space build on examples. The simplifications are mean to discard the superfluous details that are unlikely to generalize to new instances. However, to decide what data to discard and what data to keep, you must make a hypothesis. For example, a linear model makes the hypothesis that the data is fundamentally linear and that the distance between the instances and the straight line is just noise, which can safely be ignored.

If there is no hypothesis about the data, then there is no reason to prefer one surrogate over any other.  For some datasets, the best model is a linear model, while for other datasets it is a neural network. No model is a priori guaranteed to work better, this is consequences from the No Free Lunch (NFL) theorem. The only way to know for sure which model is best is to evaluate them all. Since this is not possible, in practice you make some reasonable assumptions about the data and you evaluate only a few reasonable models. For example, for simple tasks, you may evaluate linear models with various levels of regularization, and for a complex problem, you may evaluate various neural networks.

"No Free Lunch" (NFL) theorems demonstrate that if an algorithm performs well on a certain class of problems then it necessarily pays for that with degraded performance on the set of all remaining problems Additionally, the name emphasizes the parallel with similar results in supervised learning.
\begin{enumerate}
    \item You have to try multiple types of surrogate(models) to find the best one for your data.
    \item A number of NFL theorems were derived that demonstrate the danger of comparing algorithms by their performance on a small sample of problems.
\end{enumerate}

A set of models is defined that can form a partial or complete hypothesis to describe the problem.
Also during the increase of the experiments may change the model that best describes the existing problem
As a result, there is variability for each problem and configuration step at the same time. 
A set of hypotheses can solve this problem but it takes longer time for cross validation.

\paragraph{Inner interfaces} 
    Supervised learning consists in learning the link between two datasets: 
    the observed data X and an external variable y that we are trying to predict, usually called target or labels. Most often, y is a 1D array of length $n_samples$.
    All supervised estimators in scikit-learn implement a fit(X, y) method to fit the model and a predict(X) method that, given unlabeled observations X, returns the predicted labels y.

    Using arbitrary regression models from scikit-learn as surrogates

    Problem that each optimization framework/library use inner interfaces. 
    It is necessary to define a standard that implements best practices for extension libraries \cite{buitinck2013api}.
    We introduce new Model-based line for parameter tuning. 

% --------------------------------------------------------------------------------------------
% -------------------------------------------  Stage 1: Cross-Validation Surr   
% -------------------------------------------------------------------------------------------- 
\section{Validate hypothesis}
    \epigraph{``All models are wrong but some are useful``}{\textit{â€“ George Box}}

    The main task of learning algorithms is to be able to generalize to unseen data. Surrogate model as learning model should generalize examples to valid hypothesis. 
    Since we cannot immediately check the surrogate performance on new, incoming data, it is necessary to sacrifice a small portion of the examples to check the quality of the model on it.
    n case if surrogate model have enoughs score (pass metrics threshold) we consider it valid and could be processed as subject for inference(prediction).

    \subsection{Sampling strategy}
    Oversampling and undersampling in data analysis. Alleviate imbalance in the dataset. 
    Imbalance in dataset is not always a problem, more so for optimization tasks. 

    The main gain for models not to provide best accuracy on all search space but provide possible optimum regions.
    Accuracy in prediction optimal regions or points from there will direct the search in the right direction.

    Predictor variables can legitimately over- or under-sample. 
    In this case, provided a carefully check that the model assumptions seem valid.


    for other set of parameters, and make a choice from more diverse pool of models.


% --------------------------------------------------------------------------------------------
% -----------------------------------  Stage 2: Trashhold, Surr Valid, Dim Combinations       
% --------------------------------------------------------------------------------------------


% --------------------------------------------------------------------------------------------
% -------------------------------------------  Stage 3: Valid Surr Test -> Detect best Surr  
% --------------------------------------------------------------------------------------------

% --------------------------------------------------------------------------------------------
% ---------------------------------------  Stage 4: Apply MO algorithm on Surr -> Predict    
% --------------------------------------------------------------------------------------------



\section{Results}

[ref: Multi-Objective Parameter Configuration of Machine Learning Algorithms using Model-Based Optimization]
The approach is linked to the field of surrogate assisted optimizations. In many practical settings only a restricted budget is spendable. For example, the arise of Big Data confronts many machine learning techniques with new expensive parameter configuration problems. A single training of a Support Vector Machine (SVM) on a data-set containing less than a million observations can take several hours.