\chapter{Evaluation. Experimental Results} 

This chapter presents the evaluation of the proposed method on test problems with diverse objective landscape and with a various number of search variables.

MOEA is called globally convergent if the produced, non-dominated population converges to the true Pareto front while the number of generations goes to infinity.

Questions to find out:
\begin{itemize}
    \item Advantages and disadvantages of the model-based optimization over the classic MOEA
    \item Model-based optimization (MBO): compositional vs single surrogate 
    \item MBO: several identical surrogate vs surrogate portfolio
    \item Infill criteria: Selection a point from Pareto-front approximated population. Prior vs Posterior
    \item Efficacy in handling problems having more than two objectives
\end{itemize}

\cite{kouwe2018benchmarking}

% --------------------------------------------------------------------------------------------
% ------------------------------------------------     Benchmark problems      
% --------------------------------------------------------------------------------------------
\section{Benchmark problems}
For comparison was selected several widespread synthetic benchmark suites. All of them are scalable in parameters space and some in objective space also. They simulate real life problem and have main related chalanges such as multi-modality, different surface type,not uniform search space and etsetra.

    % ------------------------------------------------  ZDT      
    \paragraph{ZDT}
    This widespread test suite\cite{ZitzlerDT00} was conceived for two-objective problems and takes its name from its authors Zitzler, Deb and Thiele. Each test function involves a particular feature that is known to cause difficulty in the evolutionary optimization process, mainly in converging to the Pareto-optimal front (e.g., multimodality and deception).

    \begin{itemize}
        \item ZDT1: function has a convex Pareto-optimal front
        \item ZDT2: function has a non-convex Pareto-optimal front
        \item ZDT3: function adds a discreteness feature to the front. Its Pareto-optimal front consists of several noncontiguous convex parts. The introduction of a sine function in this objective function causes discontinuities in the Pareto-optimal front, but not in the parameter space.
        \item ZDT4: function has 21 local Pareto-optimal fronts and therefore is highly multi-modal
        \item ZDT5: integer problem
        \item ZDT6: function has a non-uniform search space: the Pareto-optimal solutions are non-uniformly distributed along the global Pareto front, and also the density of the solutions is lowest near the Pareto optimal front and highest away from the front
    \end{itemize}

    In their paper the authors propose a set of 6 different scalable problems all originating from a well thought combination of functions allowing, by construction, to measure the distance of any point to the Pareto front

    % ------------------------------------------------  DTLZ
    \paragraph{DTLZ}
    This benchmark suite\cite{DebTLZ05} was conceived for multiobjective problems with scalable fitness and objective dimensions and takes its name from its authors Deb, Thiele, Laumanns and Zitzler. All problems in this test suite are box-constrained continuous n-dimensional multi-objective problems, scalable in fitness dimension.

    \begin{itemize}
        \item DTLZ1: The optimal pareto front lies on a linear hyperplane
        \item DTLZ2: The search space is continous, unimodal and the problem is not deceptive
        \item DTLZ3: The search space is continous, unimodal and the problem is not deceptive. It is supposed to be harder to converge towards the optimal pareto front than DTLZ2
        \item DTLZ4: The search space contains a dense area of solutions next to the plane
        \item DTLZ5: This problem will test an MOEA’s ability to converge to a cruve and will also allow an easier way to visually demonstrate (just by plotting $f_M$ with any other objective function) the performance of an MOEA. Since there is a natural bias for solutions close to this Pareto-optimal curve, this problem may be easy for an algorithmn to solve. Because of its simplicity its recommended to use a higher number of objectives
        \item DTLZ6: A more difficult version of the DTLZ5 problem with the non-linear distance function g makes it harder to convergence against the pareto optimal curve
        \item DTLZ7: This problem has disconnected Pareto-optimal regions in the search space
    \end{itemize}


    % ------------------------------------------------  WFG
    \paragraph{WFG}
    This test suite \cite{WFGref} was conceived to exceed the functionalities of previously implemented test suites. In particular, non-separable problems, deceptive problems, truly degenerative problems and mixed shape Pareto front problems are thoroughly covered, as well as scalable problems in both the number of objectives and variables. Also, problems with dependencies between position and distance related parameters are covered. The WFG test suite was introduced by Simon Huband, Luigi Barone, Lyndon While, and Phil Hingston. All these problems, were developed satisfying the following guidelines:
        \begin{enumerate}
            \item A few unimodal test problems should be present in the test suite. Various Pareto optimal geometries and bias conditions should define these problems, in order to test how the convergence velocity is influenced by these aspects.
            \item The following three Pareto optimal geometries should be present in the test suite: degenerate Pareto optimal fronts, disconnected Pareto optimal fronts and disconnected Pareto optimal sets
            \item Many problems should be multimodal, and a few deceptive problems should also be covered
            \item The majority of test problems should be non-separable
            \item Both non-separable and multimodal problems should also be addressed
        \end{enumerate}

        \begin{itemize}
            \item WFG1: This problems skews the relative significance of different parameters by employing different weights in the weighted sum reduction. Also, this problem is unimodal and with a convex and mixed Pareto optimal geometry
            \item WFG2: This problem is non-separable, unimodal and with a convex and disconnected Pareto optimal geometry
            \item WFG3: This is a non-separable, unimodal problem in all its objective except for the last one, which is multimodal
            \item WFG4: This is a separable, multimodal problem with a concave Pareto optimal geometry. The multimodality of this problem has larger “hill sizes” than that of WFG9: this makes it thus more difficult.
            \item WFG5: This is a deceptive, separable problem with a concave Pareto optimal geometry.
            \item WFG6: This problem is non-separable and unimodal. Its Pareto optimal geometry is concave. The non-separable reduction of this problem makes it more difficult than that of WFG2 and WFG3
            \item WFG7: This problem is separable, unimodal and with a concave Pareto optimal geometry. This, together with WFG1, is the only problem that is both separable and unimodal.
            \item WFG8: This is a non-separable, unimodal problem with a concave Pareto optimal geometry
            \item WFG9: This is a multimodal, deceptive and non-separable problem with a concave Pareto optimal geometry. Similar to WFG6, the non-separable reduction of this problem makes it more difficult than that of WFG2 and WFG3. Also, this problem is only deceptive on its position parameters.
        \end{itemize}

\section{Conclusion}

The quality of the results obtained with X was similar to the results obtained with Y,
 but with significantly fewer exactly evaluated solutions during the optimization process. 


\paragraph{Neuroevolution of augmenting topologies}
 *Training Neural Networks (especially deep ones) is hard and has many issues (non-convex cost functions - local minima, vanishing and exploding gradients etc.).

 Training Neural Networks (NNs) with Genetic Algorithms (GAs) is not only feasible, there are some niche areas where the performance is good enough to be used frequently. A good example of this is Neuroevolution of augmenting topologies or NEAT, which a successful approach to generating controllers in simple environments, such as games.

 In the more general case though, the approach does not scale well to large, deep networks with many parameters to tune.

 Genetic algorithms and other global searches for optimal parameters are robust in ways that gradient-based algorithms are not. For instance, you could train a NN with step function activations, or any other non-differentiable activation functions. They have weaknesses elsewhere. One thing relevant in the case of GAs used for NNs, is that weight parameters are interchangeable in some combinations but heavily co-dependent in other combinations. Merging two equally good neural networks with different parameters - which you would do in cross-over in a GA - will usually result in a third network with poor performance. NEAT's success is partially in finding a way to address that issue by "growing" the NN's connections and matching them up between similar neural networks.

 Gradient-based approaches are much more efficient. In general, and not just in domain of NNs, if you can calculate gradient of a function with respect to parameters, then you can find optimal parameters faster than most other optimising techniques. An accurate gradient guarantees at least a small improvement from a single evaluation, and most other optimisers fall into a generate-and-retry paradigm which cannot make that kind of guarantee. The weakness of tending to find local optima has turned out not be a major hindrance for the loss functions in NNs, and has been tackled with some degree of success using extensions to basic gradient descent such as momentum, RPROP, Adam etc.

 In practice on a large multi-layer network, gradient methods are likely orders of magnitude faster than GA searches such as NEAT for finding network parameters. You won't find any GA-trained CNNs that solve ImageNet, or even MNIST, where the GA has found the network weights unaided. However, GAs, or at least some variants of them, are not 100'\%' ruled out. For instance this 2017 blog reviews recent papers including Large-Scale Evolution of Image Classifiers which explores using GAs to discover NN hyperparameters which is an important task in machine learning, and not very tractable using gradient-based methods.
% Test problems  
% Experimental setup
% Results
% Discussion
