\chapter{Evaluation. Experimental Results} 

This chapter presents the evaluation of the proposed method on test problems with diverse objective landscape and with a various number of search variables.

MOEA is called globally convergent if the produced, non-dominated population converges to the true Pareto front while the number of generations goes to infinity.

Questions to find out:
\begin{itemize}
    \item Advantages and disadvantages of the model-based optimization over the classic MOEA
    \item Model-based optimization (MBO): compositional vs single surrogate 
    \item MBO: several identical surrogate vs surrogate portfolio
    \item Infill criteria: Selection a point from Pareto-front approximated population. Prior vs Posterior
\end{itemize}

\cite{kouwe2018benchmarking}

\section{Test suite: ZDT}
This widespread test suite was conceived for two-objective problems and takes its name from its authors Zitzler, Deb and Thiele.
Ref[“Comparison of multiobjective evolutionary algorithms: Empirical results.”, 2000]

\section{Test suite: DTLZ}
This widespread test suite was conceived for multiobjective problems with scalable fitness dimensions and takes its name from its authors Deb, Thiele, Laumanns and Zitzler.
Ref["Scalable Test Problems for Evolutionary Multiobjective Optimization", 2005]

\section{Test suite: WFG}
This test suite was conceived to exceed the functionalities of previously implemented test suites.
In particular, non-separable problems, deceptive problems, truly degenerative problems and mixed shape Pareto front problems are thoroughly covered, 
as well as scalable problems in both the number of objectives and variables. Also, problems with dependencies between position and distance related parameters are covered.
    \begin{enumerate}

        \item A few unimodal test problems should be present in the test suite. 
        Various Pareto optimal geometries and bias conditions should define these problems, 
        in order to test how the convergence velocity is influenced by these aspects.

        \item The following three Pareto optimal geometries should be present in the test suite: 
        degenerate Pareto optimal fronts, disconnected Pareto optimal fronts and disconnected Pareto optimal sets.

        \item Many problems should be multimodal, and a few deceptive problems should also be covered.
        \item The majority of test problems should be non-separable.
        \item Both non-separable and multimodal problems should also be addressed.
    \end{enumerate}
Ref[ “A Review of Multi-Objective Test Problems and a Scalable Test Problem Toolkit”, 2006]

\section{Problem suite: CEC 2009}
Competition on “Performance Assessment of Constrained / Bound Constrained Multi-Objective Optimization Algorithms”. All problems are continuous, multi objective problems.


\section{Conclusion}

The quality of the results obtained with X was similar to the results obtained with Y,
 but with significantly fewer exactly evaluated solutions during the optimization process. 


\paragraph{Neuroevolution of augmenting topologies}
 *Training Neural Networks (especially deep ones) is hard and has many issues (non-convex cost functions - local minima, vanishing and exploding gradients etc.).

 Training Neural Networks (NNs) with Genetic Algorithms (GAs) is not only feasible, there are some niche areas where the performance is good enough to be used frequently. A good example of this is Neuroevolution of augmenting topologies or NEAT, which a successful approach to generating controllers in simple environments, such as games.

 In the more general case though, the approach does not scale well to large, deep networks with many parameters to tune.

 Genetic algorithms and other global searches for optimal parameters are robust in ways that gradient-based algorithms are not. For instance, you could train a NN with step function activations, or any other non-differentiable activation functions. They have weaknesses elsewhere. One thing relevant in the case of GAs used for NNs, is that weight parameters are interchangeable in some combinations but heavily co-dependent in other combinations. Merging two equally good neural networks with different parameters - which you would do in cross-over in a GA - will usually result in a third network with poor performance. NEAT's success is partially in finding a way to address that issue by "growing" the NN's connections and matching them up between similar neural networks.

 Gradient-based approaches are much more efficient. In general, and not just in domain of NNs, if you can calculate gradient of a function with respect to parameters, then you can find optimal parameters faster than most other optimising techniques. An accurate gradient guarantees at least a small improvement from a single evaluation, and most other optimisers fall into a generate-and-retry paradigm which cannot make that kind of guarantee. The weakness of tending to find local optima has turned out not be a major hindrance for the loss functions in NNs, and has been tackled with some degree of success using extensions to basic gradient descent such as momentum, RPROP, Adam etc.

 In practice on a large multi-layer network, gradient methods are likely orders of magnitude faster than GA searches such as NEAT for finding network parameters. You won't find any GA-trained CNNs that solve ImageNet, or even MNIST, where the GA has found the network weights unaided. However, GAs, or at least some variants of them, are not 100'\%' ruled out. For instance this 2017 blog reviews recent papers including Large-Scale Evolution of Image Classifiers which explores using GAs to discover NN hyperparameters which is an important task in machine learning, and not very tractable using gradient-based methods.
% Test problems  
% Experimental setup
% Results
% Discussion
