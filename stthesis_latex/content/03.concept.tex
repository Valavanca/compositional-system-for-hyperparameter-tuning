\chapter{Concept}
The main objective of this part is to provide a thorough treatment of multy-objective parameter tuning with evolutionary algorithm(s)


Key description how to improve solutions for problems in research questions.

Multi-objective optimizations are frequently encountered in
engineering practices. The solution techniques and parametric
selections however are usually problem-specific. \cite{DBLP:journals/corr/abs-1812-07958}

    \section{Reduce effort for multi-obj prediction/solution}
        \paragraph{Surrogate model. Hypothesis as a middleware}
        Key idea is to use hypothesis model as middleware for genetic multi-objective algorithms.
        This hypothesis could be compositional and delineate target objectives. 

    \section{Reusability in parameter tuning}
        Parameter tuning can be splitted down into steps that are common for the many/single-objective optimizations. 
        Each step in optimization workflow has variability via implemented interfaces.
        Single-objective hypotheses can be combined for multi-objective optimization with compositional design.

        API of metric-learn is compatible with scikit-learn, the leading library for machine learning in Python. 
        This allows to use all the scikit-learn routines (for pipelining, model selection, etc) with metric learning algorithms through a unified interface.

        [!TODO] Real, integer, ordinal and categorical variables.

    \section{No free lunch theorem}
        Hypothesis portfolio. +1 AutoML as self-adaptive hypothesis. +1 Scalable bayesian optimization. Related with compositional design.

    \section{Range multi-objective solution}
        Multiple score metrics. R2 score on not-dominated test set.

