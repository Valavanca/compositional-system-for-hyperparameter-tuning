\chapter{Concept}
The main objective of this part is to provide a thorough treatment of multy-objective parameter tuning with evolutionary algorithm(s)


Key description how to improve solutions for problems in research questions.

Multi-objective optimizations are frequently encountered in
engineering practices. The solution techniques and parametric
selections however are usually problem-specific. \cite{DBLP:journals/corr/abs-1812-07958}

    \section{Reduce effort for multi-obj prediction/solution}
        \paragraph{Surrogate model. Hypothesis as a middleware}
        Key idea is to use hypothesis model as middleware for genetic multi-objective algorithms.
        This hypothesis could be compositional and delineate target objectives. 

    \section{Reusability in parameter tuning}
        Parameter tuning can be splitted down into steps that are common for the many/single-objective optimizations. 
        Each step in optimization workflow has variability via implemented interfaces.
        Single-objective hypotheses can be combined for multi-objective optimization with compositional design.

        API of metric-learn is compatible with scikit-learn, the leading library for machine learning in Python. 
        This allows to use all the scikit-learn routines (for pipelining, model selection, etc) with metric learning algorithms through a unified interface.

        [!TODO] Real, integer, ordinal and categorical variables.

    \section{Surrogate portfolio}
        A Surrogate(s) is a simplified version of the examples. The simplifications are meant to discard the superfluous details that are unlikely to generalize to new instances. 
        However, to decide what data to discard and what data to keep, you must make hypothesis. 
        For example, a linear model makes the hypothesis that the data is fundamentally linear and that the distance between the instances and the straight line is just noise, 
        which can safely be ignored.

        If there is no hypothesis about the data, then there is no reason to prefer one surrogate over any other. This is called the No Free Lunch (NFL) theorem. For some datasets the best
        model is a linear model, while for other datasets it is a neural network. There is no model that is a priori guaranteed to work better (hence the name of the theorem). 
        The only way to know for sure which model is best is to evaluate them all. Since this is not possible, in practice you make some reasonable assumptions about the data 
        and you evaluate only a few reasonable models. For example, for simple tasks you may evaluate linear models with various levels of regularization, 
        and for a complex problem you may evaluate various neural networks.

        "No Free Lunch" (NFL) theorems demonstrate that if an algorithm performs well on a certain class of problems then it necessarily pays for that with degraded performance on the set of 
        all remaining problems Additionally, the name emphasizes the parallel with similar results in supervised learning.
        \begin{enumerate}
            \item You have to try multiple types of surrogate(models) to find the best one for your data.
            \item A number of NFL theorems were derived that demonstrate the danger of comparing algorithms by their performance on a small sample of problems.
        \end{enumerate}


    \section{Conclusions}
        
        \begin{enumerate}
            \item Surrogate portfolio. Search a better hypothesis for a specific problem at a particular stage of parameter tuning
            \item Large set of evaluation problems for comprehensive and fair comparison
            \item Interfaces for reusability and scalability. 
        \end{enumerate}
    

