\chapter{Introduction. Challenges and Problems.}

Multi-objective optimisation is an established parameter tuning technique. It is especially suited to solve complex, multidisciplinary design problems with an accent on system design.

When we talk about several objectives, the intention is to find good compromises rather than a single solution as in global optimization.
Since the solution for multi-objective optimization problems gives the appearance to a set of Pareto-optimal points, evolutionary optimization algorithms are ideal for handling multi-objective optimization problems.

General optimization methods could be classified into derivative and non-derivative methods. In this thesis focuses on non-derivative methods, as they are more suitable for parameter tuning. Therefore, they are also known as black-box methods and do not require any derivatives of the objective function to calculate the optimum.  Other benefits of these methods are that they are more likely to find a global optimum. 

\section{Motivation}
    The purpose of this study is to introduce new surrogate-design-criteria for multi-objective hyperparameter optimization software.

    Motivation Examples in tuning algorithms:
    \begin{itemize}
        \item Local search: neighbourhoods, perturbations, tabu length, annealing
        \item Tree search: pre-processing, data structures, branching heuristics, clause learning deletion
        \item Genetic algorithms: population size, mating scheme, crossover, mutation rate, local improvement stages, hybridizations
        \item Machine Learning: pre-processing, learning rate schedules
        \item Deep learning (in addition): layers, dropout constants, pre-training, activations functions, units/layer, weight initialization  
    \end{itemize}

\section{Objectives}
    Black-box multi-objective problems given a finite number of function evaluations

\section{Research Questions}
\begin{enumerate}
    \item RQ(Cost): Does surrogate-based optimization cheaper in evaluations than other multi-goal optimization tools?
    \item RQ(Convergence speed): Does with surrogate-based optimization solutions converge faster to Pareto-front than with other multi-goal optimization tools?
    \item RQ(Quality): Does surrogate-based optimization return similar or better solutions than other optimization tools?
    \item RQ(Extensions and reusability): Reusable compositional system for optimization. Is it possible to extend light-weight single-objective experiments to heavy-weight multi/many-objective?
\end{enumerate}

In numerous test problems, compositional-surrogate finds comparable solutions to standard MOEA (NSGA-II, MOEAD, MACO, NSPSO) doing considerably fewer evaluations (300 vs 5000). 
Surrogate-based optimization is recommended when a model is expensive to evaluate.

% \section{Solution}

% \section{Organization of the Thesis}
