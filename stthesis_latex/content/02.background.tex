\chapter{Foundation}

    In common old-fashioned software design, engineers carefully convert overall models into domain-specific tools. In this approach, designers codify the current understanding of the problem into the parameters. 

    % --------------------------------------------------------------------------------------------
    % ------------------------------------------------        Parameter tuning       -------------
    % --------------------------------------------------------------------------------------------
    \section{Parameter tuning}

        Given recent advances in computing hardware, software analysts either validate engineer models or find optimal configuration by using parameter tuning tools to explore thousands to millions of inputs for their systems. 

        In this article assume that parameter tuning is a subset problem of general, global optimizations. It's also mean that we consider some fitness function `f` that converts the parameter vector to output objectives.  Note that the term "real evaluation" or "black-box evaluation" as a synonym for the fitness function `f`.  The goal of parameter tuning as an optimization task lay on fast iterative search with improvements in each objective dimension. The term "fast" means that the convergence to global optimum is achieved with the least real evaluations and shorter time frame.

        We consider fitness function `f` as black-box with parameter and objective space. Parameter space has structure and could consist from continues and categorical dimensions. Sometimes, some combinations of parameter settings are forbidden. Each pony from parameter space lead to some point in objective space. Configurations often yield qualitatively different behavior.
        Objective space also could be described as usual objectives as accuracy, runtime, latency, performance, error rate, energy, et.s. On each objective should gain the best possible value and rich system tradeoff.

        Optimization technics:
        \begin{itemize}
            \item Grid search vs Random search
            \item Heuristics and Metaheuristic. (Simulated annealing, Evolutionary algorithm..) These methods aim at generating approximately optimal solutions in a single run. Also could operate with sets of solutions being outcomes of multiple objectives.
            \item Sequential design (Bayesian optimization, Evolutionary algorithm..) Bayesian methods differ from random or grid search in that they use past evaluation results to extrapolate and choose the next values to evaluate. Limit expensive evaluations of the objective function by choosing the next input values based on those that have done well in the past.
        \end{itemize}

        Optimization cost of black-box:
        \begin{itemize}
            \item Evaluation may be very expensive
            \item Sampling budget is unknown
            \item Possibly noisy objectives
            \item Feasibility constraints
            \item Multi-objectivity
        \end{itemize}

        Ideally, we want a method that can explore the search space while also limiting evaluations of hyperparameter choices. 
        The single criterion in parameter tuning may not be sufficient to correctly characterize the behaviour of the configuration space that is why multiple criteria have to be considered.
        One way to clarify the task of understanding the space of possible solutions is to focus on the non-dominated frontier or Pareto-front, the subset of solutions that are not worse than any other but better on at least one goal. The difficulty here is that even the Pareto frontier can be too large to understand. 
    

    % --------------------------------------------------------------------------------------------
    % ------------------------------------------------        Multi-objective       -------------
    % --------------------------------------------------------------------------------------------
    \section{Multi-objective optimization}

        Parameter tuning is present in our daily life and comes in a variety of states. The goal is the rich best possible objective by correctly choosing the system parameters. 
        Common of optimization problems requires the simultaneous optimization of multiple, usually contradictory, objectives. These type of problems are termed as multiobjective optimization problems. The solution to such problems is a family of points, that placing on a Pareto front. Knowledge of the Pareto front allows visualizing appropriate decisions in terms of performance for each objective.

        "Multi-objective optimization(MOO) deals with such conflicting objectives. It provides a
        mathematical framework to arrive at optimal design state which accommodates the various criteria demanded by
        the application. The process of optimizing systematically and simultaneously a collection of objective functions
        are called multi-objective optimization (MOO) \cite{odugod2013}".

        For a multi-objective problem, we consider "solution" as points from parameter space that lead to non-dominated results in objective space. This set of points approximate real Pareto-front. Improving "solution" means that sets of points coincide better with real Pareto-front.
        How to search for an optimal solution to the multi-objective optimization problem?

    % ------------------------------------------------        Scalarizing       -------------
        \subsection{Scalarizing. Weighted sum methods}

            Scalarizing approach is built on the traditional techniques to creating an alternative problem with a single,
            composite objective function. Single objective optimization techniques are then applied to this composite function to obtain a single optimal solution.
            The weighted-sum methods it's a well known type of scalarizing technic is applied to simplify a multiobjective problem. Concatenate the objectives into one criterion by using magic weighted sum factors. 
            The merged objective is used to evaluate and define the optimal solution.
            Weighted sum methods have difficulties in selecting proper weight especially when there is no connected a priori knowledge among objectives.
            Furthermore, Uniform distribution points in parameters space don't generate uniform distribution points on objective space. This means that we can't approximate Pareto-front completely even with multiple optimization rounds.
            Some scalarizing technics try to improve exploration of parameter space by assigning more "intelligence" aggregation to the objectives. Such solutions may be fragile. They change dramatically if we modify algorithm parameters.

            Moreover, the weighting method can not provide a solution among underparts of the Pareto surface due to “duality gap” for not convex cases. Even for convex cases, for example, in linear cases, even if we want to get a point in the middle of a line segment between two points, we hardly get a peak of Pareto surface, as long as the well-known simplex method is used. This implies that depending on the structure of the problem, the linearly weighted sum can not necessarily provide a solution as DM desires. \cite{Nakayama05}

    % ------------------------------------------------        MOEA       -------------
        \subsection{Multi-Objective Evolutionary Algorithms}

            Generating the Pareto set can be computationally expensive and is often infeasible because the complexity of the underlying volume limits exact techniques from being applicable. For this reason, a number of stochastic search strategies such as evolutionary algorithms, tabu search, simulated annealing, and ant colony optimization have been developed: they usually do not guarantee to identify optimal trade-offs but try to find a good approximation, i.e., a set of solutions whose objective vectors are (hopefully) not too far away from the optimal objective vectors \cite{EmmerichD18}.

            The evolutionary algorithm (EA) form a class of heuristic search methods that simulate the process of natural evolution.
            Using simplifications, this EA is subsequently determined by the two basic principles: selection and variation.
            While selection imitates the competition for reproduction and resources among living beings, the other principle, variation, imitates the natural ability to create ”new” living beings through recombination and mutation. Evolutionary algorithm possesses several characteristics that are desirable for problems including multiple conflicting objectives, and large and complicated search spaces. However, EA still need many evaluations of the "black box" system to solve a common multi-objective problem. This is further complicated by the fact that many such problems are very expensive. Consolidated, this makes EAs unfeasible for costly and Multy-objective problem.
            A good solution is the integration of the surrogate model which extrapolate and approximate the fitness landscape from samples. Multi-objective Evolutionary Algorithms (MOEAs) use this surrogate model as a target for optimization. Assumed that solution from surrogate nearby to a global optimum.
            The goal of this thesis is to understand if the performance of MOEAs approach can be improved by using compositional surrogates. The key idea of compositional surrogates is the splitting objective space to multiple surrogates that extrapolate it independently.Combination of multiple hypotheses should give them the potential to approximate more complicated problems. This approach avoids the idea of a single surrogate model, preferring instead to use the composition hypothesis to split out the terrain of objective space.

            The multiple surrogates are analysed on objectives with various complexity, beside the simple and complicated unimodal structure. Generating a cloud of candidates is computationally expensive.

            Evolutionary optimizers explore populations of candidate solutions in each generation, some mutator can make changes to the current population. A select operator then picks the best mutants which are then combined in some way to become generation i+1. 
            This century, there has been much new work on multi-objective evolutionary algorithms with two or three objectives 
            (as well as many-objective optimization, with many more objectives). Multi-objective Evolutionary Algorithms (MOEAs) are popular tools to solve optimization problems, because of their applicability to complex fitness landscapes and solid performance on problems with large design spaces. While other methods also exist, in this thesis we will focus on improving approaches with Evolutionary Algorithms for the Multy-objective optimizations.
            This search-based software engineering is a rapidly expanding area of research and a full survey of that work is 
            beyond the scope of this thesis.

    % ------------------------------------------------        Metrics       -------------
        \subsection{Metrics for multi-objective solution}

            In single-objective minimization, the quality of a given solution is trivial to quantify:
            the smaller the objective function value, the better. However, evaluating the quality of an approximation of a Pareto set is non trivial.
            The question is important for the comparison of algorithms or prediction next configuration.

            According to \cite{ZitzlerDT00}, a Pareto front approximation should satisfy the following:
            \begin{itemize}
                \item The distance between the Pareto front and its approximation should be minimized.
                \item A heigh distribution of the non-dominated points is desirable.
                \item The range of the approximated front should be maximized, i.e., for each objective, a wide range of values should be covered by the non-dominated points.
            \end{itemize}

            Metrics for performance indicators partitioned into four groups according to their properties \cite{Audet2018PerformanceII}: 
            \begin{itemize}
                \item cardinality
                \item convergence
                \item distribution
                \item spread
            \end{itemize}
        
            Base on the right metrics general multi-objective algorithm keep making progress toward the Pareto front in the objective function space.
            The goal of optimizing a multi-objective problem is to obtain an approximation solution set to the reference Pareto front, including the following subgoals:
            \begin{itemize}
                \item All solution set are as close as possible to the Pareto front
                \item All solution set are as diverse as possible in the objective space
                \item Evaluate as few solution as possible
            \end{itemize}
            Straightforward applying of the simple coefficient of determination (R2) is the wrong indicator of success. Evaluations of different sets of Pareto optimal points is multi-objective task.
            The necessary objectives follow for improving solutions:
            \begin{itemize}
                \item Keep hypervolume low. Reference point is 0 for all objectives.
                \item Maximize sparsity of points. Average distance. Crowding Distance. Spacing metrics.
                \item Maximize non-dominant decisions in the total population
            \end{itemize}

            Also distribution and spread indicators is consider in this work. According to \cite{CustodioMVV11}, “the spread metrics try to measure the extents of the spread achieved in a computed Pareto front approximation”. They are not useful to evaluate the convergence of an algorithm, or at comparing algorithms. They only make sense when the Pareto set is composed of several solutions.

            For multi-objective optimization (MOO), an algorithm should provide a set of solutions that realize the optimal trade-offs between the considered optimization objectives, i.e., Pareto set. Therefore, the performance comparison of MOO algorithms is based on their Pareto sets. In this study, three popular metrics are used to quantify the performance of the algorithms. 
            \begin{itemize}
                \item Hypervolume (HV)\cite{Zitzler2000ComparisonOM}. 
                This metric represents the volume of the objective space that is covered by the individuals of a non-dominated solutions set (solutions that belong to a Pareto front). The volume is delimited by two points: one point that is called the anti-optimal point (A) that is defined as the worst solution inside the objective space, and a second optimal point (pseudo-optimal) that is calculated by the proposed solution method. 
                Determining the hypervolume indicator is a computationally expensive task. Even in case of a reasonably small dimension and low number of points (e.g. 100 points in 10 dimensions), 
                there are currently no known algorithms that can yield the results fast enough for use in most multiple-objective optimizers
                \item Non-dominated Ratio (NDR). This metric employs the non-dominated count of a solution set divided by the total size of solution set. Higher values are preferred to lower ones.
                \item Spacing \cite{Schott1995FaultTD}. Describe the distribution of Pareto points. Fewer space metrics means better coverage of objectives values range.
                
            \end{itemize}

        % ------------------------------------------------        Conclusion       -------------
        \paragraph{Conclusion}

            For optimization expensive black-box:
            \begin{itemize}
                \item Scalable algorithms that convert multi-objective to single objective problem produce solution that not accurate enough(Scalarizing). Also this approach suitable for a limited type of problem.
                \item Genetic algorithms. This approach is costly to perform and not appropriate for expensive problems.
            \end{itemize}
            Optimization gap in obtaining high quality, multi/single-obj solutions in expensive to evaluate experiments.
            Experiments as a black box, derivative-free. Reference to surrogate optimization.

    \section{Surrogate optimization}

        To dealing with expensive optimization problem more quickly, we can use surrogate models in the optimization process to approximate the objective functions of the problem. Approximation of solution is faster than the whole optimization process can be accelerated. Nevertheless, the extra time needed to build and update the surrogate models during the optimization process.

        In the literature the term surrogate model (sometimes also meta-model) based optimization is 
        used where, during the optimization processes, some solutions are not evaluated with the original 
        objective function, but are approximated using a model of this function. Different modeling methods 
        are used to build the surrogate models. For single and multiobjective optimization similar methods are used. 
        These methods typically return only one approximated value, which is why in multiobjective problems several 
        models have to be used, so that every model approximates one objective. Some of the most commonly used methods 
        are the Response Surface Method [2], Radial Basis Function [3], Neural Network [4], Kriging [5] and 
        Gaussian Process Modeling [6, 7, 8]. 
        In singleobjective optimization the usage of surrogate models is well established and has proven to be successful. 
        In the literature many different algorithms and various modeling techniques are used to solve real-world and 
        benchmark problems [9, 10]). The results typically show that the surrogate-model-based optimization in 
        comparison with optimization without surrogates provides comparable results in fewer objective function 
        evaluations [11, 12].
        Within surrogate-model-based optimization algorithms a mechanism is needed to find a balance between the 
        exact and approximate evaluations. In evolutionary algorithms this mechanism is called evolution control [13] 
        and can be either fixed or adaptive. In fixed evolution control the number of exact function evaluations that 
        will be performed during the optimization is known in advance. Fixed evolution control can be further divided 
        into generation-based control, where in some generations all solutions are approximated and in the others they 
        are exactly evalu- ated [14], and individual based control, where in every generation some (usually the best) 
        solutions are exactly evaluated and others approximated [15].
        In adaptive evolution control, the number of exactly evaluated solutions is not known in advance,
        but depends on the accuracy of the model for the given problem. Adaptive evolution control can be used in one of two ways: 
        as a part of a memetic search or to pre-select the promising individuals which are then exactly evaluated [16].
        \cite{MlakarPTF15}

        Surrogate used to expedite search for global optimum. Global accuracy of surrogate
        not a priority. Surrogate model is cheaper to evaluate than the objective.

        Bayesian optimization (BO) methods often rely on the assumption that the objective function 
        is well-behaved, but in practice the objective functions are seldom well- behaved even if 
        noise-free observations can be collected. We propose to address the issue by focusing on 
        the well- behaved structure informative for search while ignoring detrimental structure 
        that is challenging to model data efficiently. [arXiv:1906.11152v2]

        robust surrogate models

        In \cite{KrallMD15} proposed approaches that apply kind of surrogate assistant to evaluations and ranging new 
        population. It allows detect the most informative examples in population and evaluate them. 
        identifies and evaluates just those most informative examples
        In the end done less evaluations of real system.Another way to explore solutions is to apply some heuristic to decompose the total space into many smaller problems, and then use a simpler optimizer for each region. For
        Another way to explore solutions is to apply some heuristic to 
        decompose the total space into many smaller problems, and then use a simpler optimizer for each region. 
        
        GP-DEMO \cite{MlakarPTF15} The algorithm is based on the newly defined relations for comparing solutions under uncertainty. 
        These relations minimize the possibility of wrongly performed comparisons of solutions due to inaccurate 
        surrogate model approximations. Using this confidence interval, we define new dominance relations that take into account 
        this uncertainty and propose a new concept for comparing solutions under uncertainty that requires exact evaluations 
        only in cases where more certainty is needed.
        surrogate-model-based MOEA.

        Kind of extend search stage of MOEA with surrogate to simulate evaluation of population. It transform
        problem of searching new better population to improving general hypothesis how and where Pareto set presented.  
        
        We could descreibe compositional-based surrogate optimization as "compound"[ref Asman] gray-box system box black-box optimization
        whit a lot of open research areas where surroagte shound improved, managing portfolio, compare of predictions Pareto fronts,
        As a developer you can focused on specific problem and don't now how implement other components. 
        this is one of the main advantage to the described approach.

        In surrogate-model-based multiobjective optimization, approximated values are often mistakenly used in the 
        solution comparison. As a consequence, exactly evaluated good solutions can be discarded from the population 
        because they appear to be dominated by the inaccurate and over-optimistic approximations. This can slow the 
        optimization process or even prevent the algorithm from finding the best solutions \cite{MlakarPTF15}.

        Surrogates are also used to rank and filter out offspring according to Pareto-related indicators like the 
        hypervolume [20], or a weighted sum of the objectives [21]. The problem with the methods that use 
        hypervolume as a way of finding promising solutions is the calculation time needed to calculate the 
        hypervolume, especially on many objectives. Another possibility is described in [22], where the authors 
        present an algorithm that calculates only non-dominated solutions or solutions that can, because of variance, 
        become non-dominated \cite{MlakarPTF15}.


        \cite{EngSurMod}        

        \paragraph{Use cases}
        Example for each type of optimization. Justification solution.
        Conclusion: Design gap in optimization/parameter tuning. 
        Need to indicate optimization workflow for expensive process/experiments. 
        The argument(s) why we need new architecture. Reference to composition architecture.

        Surrogate based optimization has proven effective in many aspects of engineering and in applications where data is "expensive", or difficult, to evaluate.


    \section{Compositional architecture}
        \paragraph{Compositional surrogates}
        Can the same single-objective models be equally applied to various types of problems in multi-/single-objective optimization?

        When there is no correlation between the outputs, a very simple way to solve this kind of problem is to build n independent models, i.e. one for each output, 
        and then to use those models to independently predict each one of the n outputs. 

        Nevertheless, it is likely that the output values related to the same input are themselves correlated, 
        but an often naive way to build multiple models to capable of predicting simultaneously all n outputs is often given good results. 


        Later research generalized this approach. MOEA/D (multiobjective evolutionary algorithm based on decompo- sition [15]) is a 
        generic framework that decomposes a multi- objective optimization problem into many smaller single problems, 
        then applies a second optimizer to each smaller subproblem, simultaneously

        Moreover, if there are more models, their errors can add up, as well as the time needed to train the models. 
        In memetic algorithms, especially if the surrogate model is not very accurate, a local optimum can be found 
        instead of the global optimum \cite{MlakarPTF15}.


        In the case of pre-selecting the promising individuals, the surrogate model is used to find the promising 
        or drop the low-quality individuals even before they are exactly evaluated, thus reducing the number of 
        exact evaluations. For example, OEGADO [19] creates a surrogate model for each of the objectives. 
        The best solutions in every objective get also approximated on other objectives, which helps with 
        finding trade-off individuals. The best individuals are then exactly evaluated and used to update the models.


        \paragraph{Interfaces and Contracts}


        \paragraph{Reusable software}
        Problem that each optimization framework/library use inner interfaces. 
        It is necessary to define a standard that implements best practices for extension libraries \cite{buitinck2013api}.

        We introduce new Model-based line for parameter tuning. 


    \section{Scope of work}
        \todo{make some nice tree-diagram}

        Describe and implement workflow for multi-objective parameter tuning of derivative free, black-box system. Parameter estimation is costly.
        The proposed solutions are also suitable for single-criteria optimization. Problem Setting.

        Goal:
        \begin{enumerate}
            \item Globally optimize an objective function(s) that is expensive to evaluate. Single/Multi-objective parameter tuning
            \item Scalability in optimization objective. Simultaneously. Gradient-free evaluation.
            \item Components reuse. Extensibility with other frameworks.
        \end{enumerate}

        Problem:
        \begin{enumerate}
            \item A large number of the target black-box evaluations.
            \item Interfaces not unify.
            \item Code duplication.
        \end{enumerate}

        Solution:
        \begin{enumerate}
            \item Compositional architecture.
            \item Surrogate optimization.
        \end{enumerate}



        Without automated tools, it can take days for experts to review just a few dozen examples.  In that same time, an automatic tool can explore thousands to millions to billions more solutions. 
        People find it an overwhelming task just to certify the correctness of conclusions generated from so many results.
