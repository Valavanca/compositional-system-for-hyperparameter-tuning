% --- MOEA
\newglossaryentry{moea}{type=\acronymtype, name={MOEA}, description={Multi-objective evolutionary algorithm}, first={Multi-objective evolutionary algorithm (MOEA)}}

% --- MOO
\newglossaryentry{moo}{type=\acronymtype, name={MOO}, description={multi-objective optimization}, first={multi-objective optimization (MOO)}}

% --- EA
\newglossaryentry{ea}{type=\acronymtype, name={EA}, description={Evolutionary algorithm}, first={Evolutionary algorithm (EA)}}

% --- DoE
\newglossaryentry{doe}{type=\acronymtype, name={DoE}, description={Design of experiment}, first={Design of experiment(DoE)}}

% --- LHS
\newglossaryentry{lhs}{type=\acronymtype, name={LHS}, description={Latin hypercube sampling}, first={Latin hypercube sampling (LHS)}}


% --- NFL
\newglossaryentry{nflg}{name={NFL},
    description={No-free-lunch (NFL) theorem\cite{NFL:DolpertM97} demonstrate that if an algorithm performs well on a certain class of problems then it necessarily pays for that with degraded performance on the set of all remaining problems Additionally, the name emphasizes the parallel with similar results in supervised learning. 1. You have to try multiple types of components to find the best one for your data; 2. A number of NFL theorems were derived that demonstrate the danger of comparing algorithms by their performance on a small sample of problems.}}
\newglossaryentry{nfl}{type=\acronymtype, name={NFL}, description={No-free-lunch theorem}, first={No-free-lunch (NFL) theorem\glsadd{nflg}}, see=[Glossary:]{nflg}}





\newglossaryentry{apig}{name={API},
    description={An Application Programming Interface (API) is a particular set
of rules and specifications that a software program can follow to access and
make use of the services and resources provided by another particular software
program that implements that API}}

%%% define the acronym and use the see= option
\newglossaryentry{api}{type=\acronymtype, name={API}, description={Application
Programming Interface}, first={Application
Programming Interface (API)\glsadd{apig}}, see=[Glossary:]{apig}}


% "No Free Lunch" (NFL) theorems demonstrate that if an algorithm performs well on a certain class of problems then it necessarily pays for that with degraded performance on the set of all remaining problems Additionally, the name emphasizes the parallel with similar results in supervised learning.