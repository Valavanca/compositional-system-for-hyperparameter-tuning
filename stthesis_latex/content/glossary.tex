% --- MOEA
\newacronym{moea}{MOEA}{Multi-objective evolutionary algorithm}

% --- MOO
\newacronym{moo}{MOO}{multi-objective optimization}

% --- MOP
\newacronym{mop}{MOP}{Multi-objective problem}

% --- EA
\newacronym{ea}{EA}{Evolutionary algorithm}

% --- DoE
\newacronym{doe}{DoE}{Design of experiment}

% --- LHS
\newacronym{lhs}{LHS}{Latin hypercube sampling}

% --- EGO
\newacronym{ego}{EGO}{Efficient global optimization}

% --- SMBO
\newacronym{smbo}{SMBO}{Sequential model-based optimization}

% --- OSI
\newacronym{osi}{OSI}{Optimization with Simulation-based Iterations}

% --- ASO
\newacronym{aso}{ASO}{Alternate Simulation-Optimization}


% --- NFL
% \newacronym{nfl}{NFL}{No-free-lunch theorem}


% sequential simulation optimization \cite{FigueiraA14}
% sequential model-based optimization \cite{JonesSW98}


%%% define the acronym and use the see= option
\newacronym[description={An Application Programming Interface (API) is a particular set
	of rules and specifications that a software program can follow to access and
	make use of the services and resources provided by another particular software
	program that implements that API}]{api}{APIs}{Application Programming Interface}

\newacronym[description={No-free-lunch (NFL) theorem\cite{NFL:DolpertM97} demonstrate that if an algorithm performs well on a certain class of problems then it necessarily pays for that with degraded performance on the set of all remaining problems Additionally, the name emphasizes the parallel with similar results in supervised learning. 1. You have to try multiple types of components to find the best one for your data; 2. A number of NFL theorems were derived that demonstrate the danger of comparing algorithms by their performance on a small sample of problems.}]{nfl}{NFL}{No-free-lunch theorem}